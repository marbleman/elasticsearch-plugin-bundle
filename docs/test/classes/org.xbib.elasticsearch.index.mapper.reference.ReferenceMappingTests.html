<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=edge"/>
<title>Test results - Class org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests</title>
<link href="../css/base-style.css" rel="stylesheet" type="text/css"/>
<link href="../css/style.css" rel="stylesheet" type="text/css"/>
<script src="../js/report.js" type="text/javascript"></script>
</head>
<body>
<div id="content">
<h1>Class org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests</h1>
<div class="breadcrumbs">
<a href="../index.html">all</a> &gt; 
<a href="../packages/org.xbib.elasticsearch.index.mapper.reference.html">org.xbib.elasticsearch.index.mapper.reference</a> &gt; ReferenceMappingTests</div>
<div id="summary">
<table>
<tr>
<td>
<div class="summaryGroup">
<table>
<tr>
<td>
<div class="infoBox" id="tests">
<div class="counter">4</div>
<p>tests</p>
</div>
</td>
<td>
<div class="infoBox" id="failures">
<div class="counter">0</div>
<p>failures</p>
</div>
</td>
<td>
<div class="infoBox" id="ignored">
<div class="counter">0</div>
<p>ignored</p>
</div>
</td>
<td>
<div class="infoBox" id="duration">
<div class="counter">35.377s</div>
<p>duration</p>
</div>
</td>
</tr>
</table>
</div>
</td>
<td>
<div class="infoBox success" id="successRate">
<div class="percent">100%</div>
<p>successful</p>
</div>
</td>
</tr>
</table>
</div>
<div id="tabs">
<ul class="tabLinks">
<li>
<a href="#tab0">Tests</a>
</li>
<li>
<a href="#tab1">Standard output</a>
</li>
</ul>
<div id="tab0" class="tab">
<h2>Tests</h2>
<table>
<thead>
<tr>
<th>Test</th>
<th>Duration</th>
<th>Result</th>
</tr>
</thead>
<tr>
<td class="success">testRefFromID</td>
<td>8.585s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testRefInDoc</td>
<td>8.607s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testRefMappings</td>
<td>8.549s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testSearch</td>
<td>9.636s</td>
<td class="success">passed</td>
</tr>
</table>
</div>
<div id="tab1" class="tab">
<h2>Standard output</h2>
<span class="code">
<pre>[22:13:50,898][INFO ][test                     ][Test worker] settings cluster name
[22:13:50,899][INFO ][test                     ][Test worker] starting nodes
[22:13:50,899][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[22:13:50,900][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[22:13:50,909][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[22:13:50,909][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [138.5gb], usable_space [138.3gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[22:13:50,909][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[22:13:50,909][INFO ][org.elasticsearch.node.Node][Test worker] node name [bafqVh9] derived from node ID [bafqVh9tQDuwoQ9zItIsUg]; set [node.name] to override
[22:13:50,909][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[5199], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[22:13:50,909][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[22:13:50,909][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[22:13:50,909][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[22:13:50,909][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[22:13:50,910][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[22:13:50,911][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[22:13:50,911][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[22:13:50,913][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:45e7:7aff:1af8:a30a prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[22:13:50,913][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[22:13:50,913][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[22:13:50,913][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[22:13:50,913][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[22:13:50,913][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[22:13:50,913][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[22:13:50,913][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[22:13:50,914][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[22:13:50,915][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[22:13:50,915][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[22:13:50,915][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[22:13:50,915][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[22:13:50,915][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[22:13:50,915][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[22:13:50,915][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[22:13:50,915][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:13:50,916][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:13:50,942][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[22:13:50,948][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[22:13:51,130][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[22:13:51,133][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[22:13:51,134][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[22:13:51,134][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[22:13:51,134][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[11]}, bound_addresses {local[11]}
[22:13:51,135][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[22:13:51,135][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [initial_join]: execute
[22:13:51,136][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[22:13:54,139][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[bafqVh9][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]}], id[77], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[22:13:54,141][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[bafqVh9][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[22:13:54,142][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[22:13:54,144][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[22:13:54,144][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] new_master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[22:13:54,144][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] publishing cluster state version [1]
[22:13:54,144][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] set local cluster state to version 1
[22:13:54,145][INFO ][org.elasticsearch.node.Node][Test worker] started
[22:13:54,145][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [2ms] done applying updated cluster_state (version: 1, uuid: 0pkpMPljTGKVPR7X-aDSuQ)
[22:13:54,146][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[22:13:54,146][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[22:13:54,146][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] publishing cluster state version [2]
[22:13:54,146][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] set local cluster state to version 2
[22:13:54,148][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[22:13:54,148][INFO ][test                     ][Test worker] nodes are started
[22:13:54,148][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [1ms] done applying updated cluster_state (version: 2, uuid: f3u9dPv8Q_ialWFO-KJxyg)
[22:13:54,148][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'test' index
[22:13:54,149][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: execute
[22:13:54,149][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating Index [[test/WqgdPT44SbGZ2Rd6n122Ug]], shards [5]/[1] - reason [create index]
[22:13:54,150][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:13:54,586][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] using dynamic[true]
[22:13:54,586][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[22:13:54,587][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[22:13:54,588][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test/WqgdPT44SbGZ2Rd6n122Ug] closing index service (reason [cleaning up after validating index on master])
[22:13:54,588][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:13:54,588][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] full cache clear, reason [close]
[22:13:54,588][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:13:54,588][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test/WqgdPT44SbGZ2Rd6n122Ug] closed... (reason [cleaning up after validating index on master])
[22:13:54,588][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [auto(index api)]]
[22:13:54,588][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] publishing cluster state version [3]
[22:13:54,588][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] set local cluster state to version 3
[22:13:54,588][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [[test/WqgdPT44SbGZ2Rd6n122Ug]] creating index
[22:13:54,588][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating Index [[test/WqgdPT44SbGZ2Rd6n122Ug]], shards [5]/[1] - reason [create index]
[22:13:54,589][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:13:55,043][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] using dynamic[true]
[22:13:55,043][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][1] creating shard
[22:13:55,044][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/WqgdPT44SbGZ2Rd6n122Ug/1, shard=[test][1]}]
[22:13:55,044][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating shard_id [test][1]
[22:13:55,044][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:55,045][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:55,046][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:55,046][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][3] creating shard
[22:13:55,046][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#3]] starting recovery from store ...
[22:13:55,046][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/WqgdPT44SbGZ2Rd6n122Ug/3, shard=[test][3]}]
[22:13:55,046][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating shard_id [test][3]
[22:13:55,047][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:55,047][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:55,047][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[bafqVh9][generic][T#3]] wipe translog location - creating new translog
[22:13:55,048][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:55,048][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][2] creating shard
[22:13:55,048][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#1]] starting recovery from store ...
[22:13:55,048][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/WqgdPT44SbGZ2Rd6n122Ug/2, shard=[test][2]}]
[22:13:55,048][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating shard_id [test][2]
[22:13:55,048][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[bafqVh9][generic][T#3]] no translog ID present in the current generation - creating one
[22:13:55,048][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:55,049][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:55,049][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[bafqVh9][generic][T#1]] wipe translog location - creating new translog
[22:13:55,049][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:55,049][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][0] creating shard
[22:13:55,049][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#2]] starting recovery from store ...
[22:13:55,050][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[bafqVh9][generic][T#1]] no translog ID present in the current generation - creating one
[22:13:55,050][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/WqgdPT44SbGZ2Rd6n122Ug/0, shard=[test][0]}]
[22:13:55,050][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating shard_id [test][0]
[22:13:55,050][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:55,050][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:55,051][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[bafqVh9][generic][T#2]] wipe translog location - creating new translog
[22:13:55,051][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:55,051][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#3]] recovery completed from [shard_store], took [7ms]
[22:13:55,051][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[test][1]], allocation id [gie4BhEYTEWt1xiwOeR_5g], primary term [0], message [after new shard recovery]]
[22:13:55,051][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [gie4BhEYTEWt1xiwOeR_5g], primary term [0], message [after new shard recovery]]
[22:13:55,051][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:55,051][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#4]] starting recovery from store ...
[22:13:55,052][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:55,052][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#1]] recovery completed from [shard_store], took [5ms]
[22:13:55,052][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#1]] [test][3] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[test][3]], allocation id [DE6yJgyDT5Svn2uToHeL9w], primary term [0], message [after new shard recovery]]
[22:13:55,052][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[bafqVh9][generic][T#2]] no translog ID present in the current generation - creating one
[22:13:55,052][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [DE6yJgyDT5Svn2uToHeL9w], primary term [0], message [after new shard recovery]]
[22:13:55,053][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: took [903ms] done applying updated cluster_state (version: 3, uuid: ncqkTYG1Rm6e6xvYlNhkGw)
[22:13:55,053][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [gie4BhEYTEWt1xiwOeR_5g], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [DE6yJgyDT5Svn2uToHeL9w], primary term [0], message [after new shard recovery]]]: execute
[22:13:55,053][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[bafqVh9tQDuwoQ9zItIsUg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=gie4BhEYTEWt1xiwOeR_5g], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:54.586Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [gie4BhEYTEWt1xiwOeR_5g], primary term [0], message [after new shard recovery]])
[22:13:55,053][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[bafqVh9tQDuwoQ9zItIsUg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=DE6yJgyDT5Svn2uToHeL9w], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:54.586Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [DE6yJgyDT5Svn2uToHeL9w], primary term [0], message [after new shard recovery]])
[22:13:55,054][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[bafqVh9][generic][T#4]] wipe translog location - creating new translog
[22:13:55,054][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:55,054][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [gie4BhEYTEWt1xiwOeR_5g], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [DE6yJgyDT5Svn2uToHeL9w], primary term [0], message [after new shard recovery]]]
[22:13:55,054][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#2]] recovery completed from [shard_store], took [6ms]
[22:13:55,054][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] publishing cluster state version [4]
[22:13:55,054][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#2]] [test][2] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[test][2]], allocation id [T4zMFQ-lTkC66hqSM0I0aA], primary term [0], message [after new shard recovery]]
[22:13:55,054][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] set local cluster state to version 4
[22:13:55,054][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#2]] [test][2] received shard started for [shard id [[test][2]], allocation id [T4zMFQ-lTkC66hqSM0I0aA], primary term [0], message [after new shard recovery]]
[22:13:55,055][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[bafqVh9][generic][T#4]] no translog ID present in the current generation - creating one
[22:13:55,056][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:55,057][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:55,057][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#4]] recovery completed from [shard_store], took [6ms]
[22:13:55,057][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:55,057][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[test][0]], allocation id [tbUVsN80T0eG05yvFbTJ-w], primary term [0], message [after new shard recovery]]
[22:13:55,057][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [tbUVsN80T0eG05yvFbTJ-w], primary term [0], message [after new shard recovery]]
[22:13:55,057][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][2] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[test][2]], allocation id [T4zMFQ-lTkC66hqSM0I0aA], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:55,057][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [T4zMFQ-lTkC66hqSM0I0aA], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:55,057][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][4] creating shard
[22:13:55,058][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/WqgdPT44SbGZ2Rd6n122Ug/4, shard=[test][4]}]
[22:13:55,058][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating shard_id [test][4]
[22:13:55,058][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:55,058][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:55,059][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:55,059][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#3]] starting recovery from store ...
[22:13:55,059][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[test][0]], allocation id [tbUVsN80T0eG05yvFbTJ-w], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:55,059][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [tbUVsN80T0eG05yvFbTJ-w], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:55,060][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[bafqVh9][generic][T#3]] wipe translog location - creating new translog
[22:13:55,060][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [gie4BhEYTEWt1xiwOeR_5g], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [DE6yJgyDT5Svn2uToHeL9w], primary term [0], message [after new shard recovery]]]: took [7ms] done applying updated cluster_state (version: 4, uuid: 5R833wzlTsKPvccF0_WwpQ)
[22:13:55,060][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [T4zMFQ-lTkC66hqSM0I0aA], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [tbUVsN80T0eG05yvFbTJ-w], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [T4zMFQ-lTkC66hqSM0I0aA], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [tbUVsN80T0eG05yvFbTJ-w], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:13:55,061][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[bafqVh9tQDuwoQ9zItIsUg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=T4zMFQ-lTkC66hqSM0I0aA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:54.586Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [T4zMFQ-lTkC66hqSM0I0aA], primary term [0], message [after new shard recovery]])
[22:13:55,061][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[bafqVh9tQDuwoQ9zItIsUg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=tbUVsN80T0eG05yvFbTJ-w], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:54.586Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [tbUVsN80T0eG05yvFbTJ-w], primary term [0], message [after new shard recovery]])
[22:13:55,061][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[bafqVh9][generic][T#3]] no translog ID present in the current generation - creating one
[22:13:55,062][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][2]], allocation id [T4zMFQ-lTkC66hqSM0I0aA], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [tbUVsN80T0eG05yvFbTJ-w], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [T4zMFQ-lTkC66hqSM0I0aA], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [tbUVsN80T0eG05yvFbTJ-w], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:13:55,062][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] publishing cluster state version [5]
[22:13:55,062][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] set local cluster state to version 5
[22:13:55,062][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:55,063][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#3]] recovery completed from [shard_store], took [5ms]
[22:13:55,063][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:55,063][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[test][4]], allocation id [ukxLE-WmRVOWYHmPIpzbQQ], primary term [0], message [after new shard recovery]]
[22:13:55,063][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[test][4]], allocation id [ukxLE-WmRVOWYHmPIpzbQQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:55,063][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [ukxLE-WmRVOWYHmPIpzbQQ], primary term [0], message [after new shard recovery]]
[22:13:55,063][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [ukxLE-WmRVOWYHmPIpzbQQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:55,063][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:55,064][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [T4zMFQ-lTkC66hqSM0I0aA], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [tbUVsN80T0eG05yvFbTJ-w], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [T4zMFQ-lTkC66hqSM0I0aA], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [tbUVsN80T0eG05yvFbTJ-w], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [3ms] done applying updated cluster_state (version: 5, uuid: OInxAdKyQp2_kAEGQtGriA)
[22:13:55,064][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [ukxLE-WmRVOWYHmPIpzbQQ], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [ukxLE-WmRVOWYHmPIpzbQQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:13:55,064][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[bafqVh9tQDuwoQ9zItIsUg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=ukxLE-WmRVOWYHmPIpzbQQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:54.586Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [ukxLE-WmRVOWYHmPIpzbQQ], primary term [0], message [after new shard recovery]])
[22:13:55,065][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [ukxLE-WmRVOWYHmPIpzbQQ], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [ukxLE-WmRVOWYHmPIpzbQQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:13:55,065][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] publishing cluster state version [6]
[22:13:55,065][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] set local cluster state to version 6
[22:13:55,066][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:55,067][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [ukxLE-WmRVOWYHmPIpzbQQ], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [ukxLE-WmRVOWYHmPIpzbQQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 6, uuid: y0_ExNe0QAyxcZ7lhyiRRQ)
[22:13:55,070][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[22:13:55,513][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] using dynamic[true]
[22:13:55,515][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [test/WqgdPT44SbGZ2Rd6n122Ug] create_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:13:55,515][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] cluster state updated, version [7], source [put-mapping[test]]
[22:13:55,515][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] publishing cluster state version [7]
[22:13:55,515][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] set local cluster state to version 7
[22:13:55,516][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [[test/WqgdPT44SbGZ2Rd6n122Ug]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:13:55,518][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [447ms] done applying updated cluster_state (version: 7, uuid: Cg48gGrGTSeUPVhzEOjM0g)
[22:13:55,543][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'authorities' index
[22:13:55,544][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: execute
[22:13:55,545][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating Index [[authorities/tNzJ1nyDRPeHG-zGMhvMBw]], shards [5]/[1] - reason [create index]
[22:13:55,545][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:13:55,998][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] using dynamic[true]
[22:13:55,998][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[22:13:56,000][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities] closing ... (reason [cleaning up after validating index on master])
[22:13:56,000][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities/tNzJ1nyDRPeHG-zGMhvMBw] closing index service (reason [cleaning up after validating index on master])
[22:13:56,000][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:13:56,000][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] full cache clear, reason [close]
[22:13:56,000][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:13:56,000][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities/tNzJ1nyDRPeHG-zGMhvMBw] closed... (reason [cleaning up after validating index on master])
[22:13:56,001][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] cluster state updated, version [8], source [create-index [authorities], cause [auto(index api)]]
[22:13:56,001][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] publishing cluster state version [8]
[22:13:56,001][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] set local cluster state to version 8
[22:13:56,001][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [[authorities/tNzJ1nyDRPeHG-zGMhvMBw]] creating index
[22:13:56,001][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating Index [[authorities/tNzJ1nyDRPeHG-zGMhvMBw]], shards [5]/[1] - reason [create index]
[22:13:56,002][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:13:56,444][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] using dynamic[true]
[22:13:56,444][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][1] creating shard
[22:13:56,444][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/tNzJ1nyDRPeHG-zGMhvMBw/1, shard=[authorities][1]}]
[22:13:56,444][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating shard_id [authorities][1]
[22:13:56,445][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:56,445][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:56,447][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:56,447][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][3] creating shard
[22:13:56,447][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#1]] starting recovery from store ...
[22:13:56,447][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/tNzJ1nyDRPeHG-zGMhvMBw/3, shard=[authorities][3]}]
[22:13:56,447][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating shard_id [authorities][3]
[22:13:56,447][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:56,447][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:56,448][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[bafqVh9][generic][T#1]] wipe translog location - creating new translog
[22:13:56,448][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:56,448][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][2] creating shard
[22:13:56,448][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#2]] starting recovery from store ...
[22:13:56,448][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/tNzJ1nyDRPeHG-zGMhvMBw/2, shard=[authorities][2]}]
[22:13:56,448][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating shard_id [authorities][2]
[22:13:56,448][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[bafqVh9][generic][T#1]] no translog ID present in the current generation - creating one
[22:13:56,449][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:56,449][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:56,449][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[bafqVh9][generic][T#2]] wipe translog location - creating new translog
[22:13:56,450][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:56,450][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][0] creating shard
[22:13:56,450][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#4]] starting recovery from store ...
[22:13:56,450][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[bafqVh9][generic][T#2]] no translog ID present in the current generation - creating one
[22:13:56,450][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/tNzJ1nyDRPeHG-zGMhvMBw/0, shard=[authorities][0]}]
[22:13:56,450][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating shard_id [authorities][0]
[22:13:56,451][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:56,451][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[bafqVh9][generic][T#4]] wipe translog location - creating new translog
[22:13:56,451][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:56,451][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:56,451][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#1]] recovery completed from [shard_store], took [6ms]
[22:13:56,451][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#1]] [authorities][1] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[authorities][1]], allocation id [sSjrcYzdQTWZcDNZS0TU2w], primary term [0], message [after new shard recovery]]
[22:13:56,451][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#1]] [authorities][1] received shard started for [shard id [[authorities][1]], allocation id [sSjrcYzdQTWZcDNZS0TU2w], primary term [0], message [after new shard recovery]]
[22:13:56,451][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[bafqVh9][generic][T#4]] no translog ID present in the current generation - creating one
[22:13:56,451][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:56,452][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#3]] starting recovery from store ...
[22:13:56,452][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:56,452][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#2]] recovery completed from [shard_store], took [5ms]
[22:13:56,452][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#2]] [authorities][3] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[authorities][3]], allocation id [w4MGsfPbSXCvgp46v5Guvw], primary term [0], message [after new shard recovery]]
[22:13:56,452][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#2]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [w4MGsfPbSXCvgp46v5Guvw], primary term [0], message [after new shard recovery]]
[22:13:56,453][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[bafqVh9][generic][T#3]] wipe translog location - creating new translog
[22:13:56,453][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: took [908ms] done applying updated cluster_state (version: 8, uuid: 0cSfNQXqQN62LFa8Lm9Hnw)
[22:13:56,453][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [sSjrcYzdQTWZcDNZS0TU2w], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [w4MGsfPbSXCvgp46v5Guvw], primary term [0], message [after new shard recovery]]]: execute
[22:13:56,453][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][1] starting shard [authorities][1], node[bafqVh9tQDuwoQ9zItIsUg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=sSjrcYzdQTWZcDNZS0TU2w], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:55.998Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][1]], allocation id [sSjrcYzdQTWZcDNZS0TU2w], primary term [0], message [after new shard recovery]])
[22:13:56,453][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][3] starting shard [authorities][3], node[bafqVh9tQDuwoQ9zItIsUg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=w4MGsfPbSXCvgp46v5Guvw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:55.998Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][3]], allocation id [w4MGsfPbSXCvgp46v5Guvw], primary term [0], message [after new shard recovery]])
[22:13:56,454][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:56,454][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#4]] recovery completed from [shard_store], took [5ms]
[22:13:56,454][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#4]] [authorities][2] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[authorities][2]], allocation id [S2ssquqcQniSM_ivU9fKBQ], primary term [0], message [after new shard recovery]]
[22:13:56,454][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#4]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [S2ssquqcQniSM_ivU9fKBQ], primary term [0], message [after new shard recovery]]
[22:13:56,454][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[bafqVh9][generic][T#3]] no translog ID present in the current generation - creating one
[22:13:56,456][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[authorities][1]], allocation id [sSjrcYzdQTWZcDNZS0TU2w], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [w4MGsfPbSXCvgp46v5Guvw], primary term [0], message [after new shard recovery]]]
[22:13:56,457][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] publishing cluster state version [9]
[22:13:56,457][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] set local cluster state to version 9
[22:13:56,458][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:56,458][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#3]] recovery completed from [shard_store], took [8ms]
[22:13:56,458][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#3]] [authorities][0] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[authorities][0]], allocation id [-dYL7fwmSoyeQOHYIT_KXQ], primary term [0], message [after new shard recovery]]
[22:13:56,459][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:56,459][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#3]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [-dYL7fwmSoyeQOHYIT_KXQ], primary term [0], message [after new shard recovery]]
[22:13:56,459][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:56,459][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][2] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[authorities][2]], allocation id [S2ssquqcQniSM_ivU9fKBQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:56,459][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [S2ssquqcQniSM_ivU9fKBQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:56,460][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][4] creating shard
[22:13:56,460][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/tNzJ1nyDRPeHG-zGMhvMBw/4, shard=[authorities][4]}]
[22:13:56,460][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] creating shard_id [authorities][4]
[22:13:56,461][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:56,461][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:56,462][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:56,462][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][0] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[authorities][0]], allocation id [-dYL7fwmSoyeQOHYIT_KXQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:56,462][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#1]] starting recovery from store ...
[22:13:56,462][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [-dYL7fwmSoyeQOHYIT_KXQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:56,464][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[bafqVh9][generic][T#1]] wipe translog location - creating new translog
[22:13:56,464][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [sSjrcYzdQTWZcDNZS0TU2w], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [w4MGsfPbSXCvgp46v5Guvw], primary term [0], message [after new shard recovery]]]: took [10ms] done applying updated cluster_state (version: 9, uuid: QzJdvTHHQ0ynbd-VWeVgZw)
[22:13:56,464][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [S2ssquqcQniSM_ivU9fKBQ], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [-dYL7fwmSoyeQOHYIT_KXQ], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [S2ssquqcQniSM_ivU9fKBQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [-dYL7fwmSoyeQOHYIT_KXQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:13:56,464][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][2] starting shard [authorities][2], node[bafqVh9tQDuwoQ9zItIsUg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=S2ssquqcQniSM_ivU9fKBQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:55.998Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][2]], allocation id [S2ssquqcQniSM_ivU9fKBQ], primary term [0], message [after new shard recovery]])
[22:13:56,464][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][0] starting shard [authorities][0], node[bafqVh9tQDuwoQ9zItIsUg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=-dYL7fwmSoyeQOHYIT_KXQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:55.998Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][0]], allocation id [-dYL7fwmSoyeQOHYIT_KXQ], primary term [0], message [after new shard recovery]])
[22:13:56,464][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[bafqVh9][generic][T#1]] no translog ID present in the current generation - creating one
[22:13:56,466][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[authorities][2]], allocation id [S2ssquqcQniSM_ivU9fKBQ], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [-dYL7fwmSoyeQOHYIT_KXQ], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [S2ssquqcQniSM_ivU9fKBQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [-dYL7fwmSoyeQOHYIT_KXQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:13:56,466][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] publishing cluster state version [10]
[22:13:56,466][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] set local cluster state to version 10
[22:13:56,468][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:56,468][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][generic][T#1]] recovery completed from [shard_store], took [8ms]
[22:13:56,468][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[authorities][4]], allocation id [4uTvbWV1Tnm3siKIB-7xmA], primary term [0], message [after new shard recovery]]
[22:13:56,468][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][generic][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [4uTvbWV1Tnm3siKIB-7xmA], primary term [0], message [after new shard recovery]]
[22:13:56,468][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:56,468][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [bafqVh9tQDuwoQ9zItIsUg] for shard entry [shard id [[authorities][4]], allocation id [4uTvbWV1Tnm3siKIB-7xmA], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:56,468][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [4uTvbWV1Tnm3siKIB-7xmA], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:56,469][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:56,470][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [S2ssquqcQniSM_ivU9fKBQ], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [-dYL7fwmSoyeQOHYIT_KXQ], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [S2ssquqcQniSM_ivU9fKBQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [-dYL7fwmSoyeQOHYIT_KXQ], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [5ms] done applying updated cluster_state (version: 10, uuid: Ytzk1NOaTwW6WWdapCSkOw)
[22:13:56,470][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [4uTvbWV1Tnm3siKIB-7xmA], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [4uTvbWV1Tnm3siKIB-7xmA], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:13:56,470][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities][4] starting shard [authorities][4], node[bafqVh9tQDuwoQ9zItIsUg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=4uTvbWV1Tnm3siKIB-7xmA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:55.998Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[authorities][4]], allocation id [4uTvbWV1Tnm3siKIB-7xmA], primary term [0], message [after new shard recovery]])
[22:13:56,472][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] cluster state updated, version [11], source [shard-started[shard id [[authorities][4]], allocation id [4uTvbWV1Tnm3siKIB-7xmA], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [4uTvbWV1Tnm3siKIB-7xmA], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:13:56,472][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] publishing cluster state version [11]
[22:13:56,472][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] set local cluster state to version 11
[22:13:56,473][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:56,474][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [4uTvbWV1Tnm3siKIB-7xmA], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [4uTvbWV1Tnm3siKIB-7xmA], primary term [0], message [master {bafqVh9}{bafqVh9tQDuwoQ9zItIsUg}{zy99WgphQ0ejryErAIbb9w}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [3ms] done applying updated cluster_state (version: 11, uuid: 54QzpoKLRQCG9h-bmst5hw)
[22:13:56,477][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: execute
[22:13:56,927][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] using dynamic[true]
[22:13:56,929][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [authorities/tNzJ1nyDRPeHG-zGMhvMBw] create_mapping [persons] with source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:13:56,930][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] cluster state updated, version [12], source [put-mapping[persons]]
[22:13:56,930][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] publishing cluster state version [12]
[22:13:56,930][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] set local cluster state to version 12
[22:13:56,931][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] [[authorities/tNzJ1nyDRPeHG-zGMhvMBw]] adding mapping [persons], source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:13:56,933][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[bafqVh9][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: took [456ms] done applying updated cluster_state (version: 12, uuid: KBHVkB-hR8ukLcqR11xj0w)
[22:13:57,405][DEBUG][org.elasticsearch.index.mapper.MapperService][Test worker] using dynamic[true]
[22:13:57,407][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _source = null
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _type = someType
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _type = null
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _uid = someType#1
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _version = -1
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings someField = 1234
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings ref = a
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings ref = b
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings ref = c
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _all = 1234
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _source
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _type
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _type
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _uid
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _version
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = someField
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = ref
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = ref
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = ref
[22:13:57,408][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _all
[22:13:57,410][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _source = null
[22:13:57,410][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _type = someType
[22:13:57,410][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _type = null
[22:13:57,410][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _uid = someType#1
[22:13:57,410][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _version = -1
[22:13:57,410][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings someField = 1234
[22:13:57,410][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings ref = a
[22:13:57,410][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings ref = b
[22:13:57,410][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings ref = c
[22:13:57,410][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _all = 1234
[22:13:57,410][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _source
[22:13:57,410][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _type
[22:13:57,411][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _type
[22:13:57,411][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _uid
[22:13:57,411][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _version
[22:13:57,411][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = someField
[22:13:57,411][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = ref
[22:13:57,411][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = ref
[22:13:57,411][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = ref
[22:13:57,411][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _all
[22:13:57,411][INFO ][test                     ][Test worker] stopping nodes
[22:13:57,411][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[22:13:57,412][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities] closing ... (reason [shutdown])
[22:13:57,412][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/tNzJ1nyDRPeHG-zGMhvMBw] closing index service (reason [shutdown])
[22:13:57,412][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[22:13:57,412][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test] closing ... (reason [shutdown])
[22:13:57,412][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:57,412][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:13:57,412][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/WqgdPT44SbGZ2Rd6n122Ug] closing index service (reason [shutdown])
[22:13:57,412][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:13:57,412][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:13:57,412][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[22:13:57,413][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:13:57,413][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:57,413][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:57,414][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:57,414][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:57,414][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:13:57,414][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:57,414][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:13:57,414][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[22:13:57,414][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[22:13:57,415][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:57,415][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:57,415][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:57,415][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:13:57,415][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[22:13:57,415][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[22:13:57,415][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:13:57,415][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:13:57,415][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:57,415][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:57,415][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:13:57,415][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:57,415][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:57,416][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:57,416][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:13:57,417][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:13:57,417][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[22:13:57,417][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:57,417][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[22:13:57,417][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:57,417][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:57,417][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[22:13:57,417][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:13:57,417][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[22:13:57,417][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:13:57,417][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:13:57,417][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:57,417][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:57,417][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:13:57,417][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:57,418][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:57,419][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:57,419][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:13:57,419][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:13:57,419][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[22:13:57,420][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[22:13:57,420][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:57,420][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:57,420][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:13:57,420][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:57,420][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[22:13:57,420][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[22:13:57,420][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:57,420][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:57,420][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:57,420][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:57,421][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:57,422][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:57,422][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:57,422][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[22:13:57,422][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[22:13:57,422][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:57,422][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:57,429][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:57,429][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:57,429][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:57,430][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:13:57,430][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:13:57,430][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:57,430][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:13:57,430][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:57,430][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[22:13:57,430][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:13:57,430][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[22:13:57,431][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:13:57,431][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/WqgdPT44SbGZ2Rd6n122Ug] closed... (reason [shutdown])
[22:13:57,431][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:13:57,431][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:13:57,431][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[22:13:57,431][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[22:13:57,431][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:57,432][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:13:57,432][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:13:57,432][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:13:57,432][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:13:57,433][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:13:57,433][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:13:57,433][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[22:13:57,433][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[22:13:57,433][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[22:13:57,433][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[22:13:57,433][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/tNzJ1nyDRPeHG-zGMhvMBw] closed... (reason [shutdown])
[22:13:57,433][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[22:13:57,433][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[22:13:57,435][INFO ][org.elasticsearch.node.Node][Test worker] closed
[22:13:57,445][INFO ][test                     ][Test worker] data files wiped
[22:13:59,447][INFO ][test                     ][Test worker] settings cluster name
[22:13:59,447][INFO ][test                     ][Test worker] starting nodes
[22:13:59,447][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[22:13:59,448][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[22:13:59,461][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[22:13:59,461][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [138.5gb], usable_space [138.3gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[22:13:59,461][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[22:13:59,461][INFO ][org.elasticsearch.node.Node][Test worker] node name [XF0WGbn] derived from node ID [XF0WGbn-Qt-AlZKx5ywYQg]; set [node.name] to override
[22:13:59,461][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[5199], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[22:13:59,461][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[22:13:59,462][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[22:13:59,462][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[22:13:59,462][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[22:13:59,462][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[22:13:59,462][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[22:13:59,462][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[22:13:59,462][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[22:13:59,462][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[22:13:59,463][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[22:13:59,463][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[22:13:59,463][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[22:13:59,463][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[22:13:59,463][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[22:13:59,463][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[22:13:59,463][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[22:13:59,463][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[22:13:59,463][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[22:13:59,463][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[22:13:59,465][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:45e7:7aff:1af8:a30a prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[22:13:59,465][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[22:13:59,465][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[22:13:59,466][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[22:13:59,466][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[22:13:59,466][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[22:13:59,466][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[22:13:59,466][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[22:13:59,466][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[22:13:59,467][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[22:13:59,467][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[22:13:59,467][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[22:13:59,468][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[22:13:59,468][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[22:13:59,468][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[22:13:59,468][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[22:13:59,468][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:13:59,468][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:13:59,493][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[22:13:59,499][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[22:13:59,685][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[22:13:59,688][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[22:13:59,689][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[22:13:59,689][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[22:13:59,689][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[12]}, bound_addresses {local[12]}
[22:13:59,690][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[22:13:59,690][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [initial_join]: execute
[22:13:59,690][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[22:14:02,695][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[XF0WGbn][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]}], id[84], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[22:14:02,696][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[XF0WGbn][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[22:14:02,696][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[22:14:02,696][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[22:14:02,696][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] new_master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[22:14:02,696][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] publishing cluster state version [1]
[22:14:02,697][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] set local cluster state to version 1
[22:14:02,697][INFO ][org.elasticsearch.node.Node][Test worker] started
[22:14:02,697][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [1ms] done applying updated cluster_state (version: 1, uuid: eqSfg2blT3i-ApvEzfzAgA)
[22:14:02,698][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[22:14:02,698][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[22:14:02,698][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] publishing cluster state version [2]
[22:14:02,698][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] set local cluster state to version 2
[22:14:02,700][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[22:14:02,700][INFO ][test                     ][Test worker] nodes are started
[22:14:02,700][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [1ms] done applying updated cluster_state (version: 2, uuid: ttJTZDMuQBehyZTMuvhT_Q)
[22:14:02,700][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'test' index
[22:14:02,700][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: execute
[22:14:02,701][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating Index [[test/iQ_ETgOoSTSwD63U_ZbfQA]], shards [5]/[1] - reason [create index]
[22:14:02,701][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:03,160][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:03,160][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[22:14:03,161][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[22:14:03,161][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test/iQ_ETgOoSTSwD63U_ZbfQA] closing index service (reason [cleaning up after validating index on master])
[22:14:03,161][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:03,161][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] full cache clear, reason [close]
[22:14:03,161][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:03,161][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test/iQ_ETgOoSTSwD63U_ZbfQA] closed... (reason [cleaning up after validating index on master])
[22:14:03,162][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [auto(index api)]]
[22:14:03,162][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] publishing cluster state version [3]
[22:14:03,162][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] set local cluster state to version 3
[22:14:03,162][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [[test/iQ_ETgOoSTSwD63U_ZbfQA]] creating index
[22:14:03,162][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating Index [[test/iQ_ETgOoSTSwD63U_ZbfQA]], shards [5]/[1] - reason [create index]
[22:14:03,162][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:03,613][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:03,614][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][1] creating shard
[22:14:03,614][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iQ_ETgOoSTSwD63U_ZbfQA/1, shard=[test][1]}]
[22:14:03,614][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating shard_id [test][1]
[22:14:03,615][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:03,615][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:03,616][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:03,616][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][3] creating shard
[22:14:03,616][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#3]] starting recovery from store ...
[22:14:03,617][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iQ_ETgOoSTSwD63U_ZbfQA/3, shard=[test][3]}]
[22:14:03,617][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating shard_id [test][3]
[22:14:03,617][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:03,617][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:03,618][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XF0WGbn][generic][T#3]] wipe translog location - creating new translog
[22:14:03,618][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:03,618][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][2] creating shard
[22:14:03,618][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#1]] starting recovery from store ...
[22:14:03,618][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iQ_ETgOoSTSwD63U_ZbfQA/2, shard=[test][2]}]
[22:14:03,618][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating shard_id [test][2]
[22:14:03,618][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XF0WGbn][generic][T#3]] no translog ID present in the current generation - creating one
[22:14:03,619][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:03,619][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XF0WGbn][generic][T#1]] wipe translog location - creating new translog
[22:14:03,619][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:03,620][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:03,620][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XF0WGbn][generic][T#1]] no translog ID present in the current generation - creating one
[22:14:03,620][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][0] creating shard
[22:14:03,620][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#2]] starting recovery from store ...
[22:14:03,620][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iQ_ETgOoSTSwD63U_ZbfQA/0, shard=[test][0]}]
[22:14:03,620][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating shard_id [test][0]
[22:14:03,621][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:03,621][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:03,621][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#3]] recovery completed from [shard_store], took [6ms]
[22:14:03,621][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[test][1]], allocation id [jyVyWzOvTuCwV1Bxf91XLQ], primary term [0], message [after new shard recovery]]
[22:14:03,621][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:03,621][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [jyVyWzOvTuCwV1Bxf91XLQ], primary term [0], message [after new shard recovery]]
[22:14:03,621][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XF0WGbn][generic][T#2]] wipe translog location - creating new translog
[22:14:03,622][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:03,622][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#1]] recovery completed from [shard_store], took [5ms]
[22:14:03,622][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:03,622][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#1]] [test][3] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[test][3]], allocation id [mTS45TAnQ8uVq_B4xFCFqw], primary term [0], message [after new shard recovery]]
[22:14:03,622][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [mTS45TAnQ8uVq_B4xFCFqw], primary term [0], message [after new shard recovery]]
[22:14:03,622][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#4]] starting recovery from store ...
[22:14:03,622][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XF0WGbn][generic][T#2]] no translog ID present in the current generation - creating one
[22:14:03,623][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: took [922ms] done applying updated cluster_state (version: 3, uuid: gB51XBo_QryEaE-ag2QPhw)
[22:14:03,623][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [jyVyWzOvTuCwV1Bxf91XLQ], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [mTS45TAnQ8uVq_B4xFCFqw], primary term [0], message [after new shard recovery]]]: execute
[22:14:03,623][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[XF0WGbn-Qt-AlZKx5ywYQg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=jyVyWzOvTuCwV1Bxf91XLQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:03.160Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [jyVyWzOvTuCwV1Bxf91XLQ], primary term [0], message [after new shard recovery]])
[22:14:03,623][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XF0WGbn][generic][T#4]] wipe translog location - creating new translog
[22:14:03,623][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[XF0WGbn-Qt-AlZKx5ywYQg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=mTS45TAnQ8uVq_B4xFCFqw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:03.160Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [mTS45TAnQ8uVq_B4xFCFqw], primary term [0], message [after new shard recovery]])
[22:14:03,624][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:03,624][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XF0WGbn][generic][T#4]] no translog ID present in the current generation - creating one
[22:14:03,624][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#2]] recovery completed from [shard_store], took [5ms]
[22:14:03,624][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#2]] [test][2] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[test][2]], allocation id [m21JIGzXQDSBsMEcY-KmGQ], primary term [0], message [after new shard recovery]]
[22:14:03,624][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#2]] [test][2] received shard started for [shard id [[test][2]], allocation id [m21JIGzXQDSBsMEcY-KmGQ], primary term [0], message [after new shard recovery]]
[22:14:03,624][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [jyVyWzOvTuCwV1Bxf91XLQ], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [mTS45TAnQ8uVq_B4xFCFqw], primary term [0], message [after new shard recovery]]]
[22:14:03,624][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] publishing cluster state version [4]
[22:14:03,624][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] set local cluster state to version 4
[22:14:03,626][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:03,626][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:03,626][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#4]] recovery completed from [shard_store], took [5ms]
[22:14:03,626][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[test][0]], allocation id [B1aMd_xQRteXDNR1b94H5g], primary term [0], message [after new shard recovery]]
[22:14:03,626][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:03,626][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [B1aMd_xQRteXDNR1b94H5g], primary term [0], message [after new shard recovery]]
[22:14:03,626][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][2] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[test][2]], allocation id [m21JIGzXQDSBsMEcY-KmGQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:03,626][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [m21JIGzXQDSBsMEcY-KmGQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:03,626][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][4] creating shard
[22:14:03,627][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iQ_ETgOoSTSwD63U_ZbfQA/4, shard=[test][4]}]
[22:14:03,627][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating shard_id [test][4]
[22:14:03,627][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:03,627][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:03,628][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:03,628][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#3]] starting recovery from store ...
[22:14:03,628][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[test][0]], allocation id [B1aMd_xQRteXDNR1b94H5g], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:03,628][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [B1aMd_xQRteXDNR1b94H5g], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:03,630][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XF0WGbn][generic][T#3]] wipe translog location - creating new translog
[22:14:03,630][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [jyVyWzOvTuCwV1Bxf91XLQ], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [mTS45TAnQ8uVq_B4xFCFqw], primary term [0], message [after new shard recovery]]]: took [7ms] done applying updated cluster_state (version: 4, uuid: UG_Uf6tgRByThEYLeT7C3Q)
[22:14:03,630][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [m21JIGzXQDSBsMEcY-KmGQ], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [B1aMd_xQRteXDNR1b94H5g], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [m21JIGzXQDSBsMEcY-KmGQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [B1aMd_xQRteXDNR1b94H5g], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:03,630][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[XF0WGbn-Qt-AlZKx5ywYQg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=m21JIGzXQDSBsMEcY-KmGQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:03.160Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [m21JIGzXQDSBsMEcY-KmGQ], primary term [0], message [after new shard recovery]])
[22:14:03,630][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[XF0WGbn-Qt-AlZKx5ywYQg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=B1aMd_xQRteXDNR1b94H5g], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:03.160Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [B1aMd_xQRteXDNR1b94H5g], primary term [0], message [after new shard recovery]])
[22:14:03,631][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XF0WGbn][generic][T#3]] no translog ID present in the current generation - creating one
[22:14:03,631][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][2]], allocation id [m21JIGzXQDSBsMEcY-KmGQ], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [B1aMd_xQRteXDNR1b94H5g], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [m21JIGzXQDSBsMEcY-KmGQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [B1aMd_xQRteXDNR1b94H5g], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:03,631][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] publishing cluster state version [5]
[22:14:03,631][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] set local cluster state to version 5
[22:14:03,633][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:03,633][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:03,633][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#3]] recovery completed from [shard_store], took [6ms]
[22:14:03,633][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[test][4]], allocation id [aj6HHFUgTeaaso4_bZsP5Q], primary term [0], message [after new shard recovery]]
[22:14:03,633][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[test][4]], allocation id [aj6HHFUgTeaaso4_bZsP5Q], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:03,633][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [aj6HHFUgTeaaso4_bZsP5Q], primary term [0], message [after new shard recovery]]
[22:14:03,633][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [aj6HHFUgTeaaso4_bZsP5Q], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:03,633][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:03,634][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [m21JIGzXQDSBsMEcY-KmGQ], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [B1aMd_xQRteXDNR1b94H5g], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [m21JIGzXQDSBsMEcY-KmGQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [B1aMd_xQRteXDNR1b94H5g], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [3ms] done applying updated cluster_state (version: 5, uuid: FUcCmgE4QFatHhiIQvC3NQ)
[22:14:03,634][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [aj6HHFUgTeaaso4_bZsP5Q], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [aj6HHFUgTeaaso4_bZsP5Q], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:03,634][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[XF0WGbn-Qt-AlZKx5ywYQg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=aj6HHFUgTeaaso4_bZsP5Q], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:03.160Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [aj6HHFUgTeaaso4_bZsP5Q], primary term [0], message [after new shard recovery]])
[22:14:03,635][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [aj6HHFUgTeaaso4_bZsP5Q], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [aj6HHFUgTeaaso4_bZsP5Q], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:03,635][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] publishing cluster state version [6]
[22:14:03,635][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] set local cluster state to version 6
[22:14:03,636][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:03,637][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [aj6HHFUgTeaaso4_bZsP5Q], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [aj6HHFUgTeaaso4_bZsP5Q], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 6, uuid: B8OyBwf0Tn2NCENMr91YjA)
[22:14:03,640][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[22:14:04,111][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:04,112][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [test/iQ_ETgOoSTSwD63U_ZbfQA] create_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:04,112][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] cluster state updated, version [7], source [put-mapping[test]]
[22:14:04,112][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] publishing cluster state version [7]
[22:14:04,112][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] set local cluster state to version 7
[22:14:04,113][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [[test/iQ_ETgOoSTSwD63U_ZbfQA]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:04,115][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [475ms] done applying updated cluster_state (version: 7, uuid: v4A2IMM2TkObST4B8t2gXw)
[22:14:04,137][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'authorities' index
[22:14:04,138][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: execute
[22:14:04,138][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating Index [[authorities/9lk82yMSTw6kl29yEW9PkQ]], shards [5]/[1] - reason [create index]
[22:14:04,138][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:04,569][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:04,570][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[22:14:04,571][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities] closing ... (reason [cleaning up after validating index on master])
[22:14:04,571][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities/9lk82yMSTw6kl29yEW9PkQ] closing index service (reason [cleaning up after validating index on master])
[22:14:04,571][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:04,571][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] full cache clear, reason [close]
[22:14:04,571][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:04,571][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities/9lk82yMSTw6kl29yEW9PkQ] closed... (reason [cleaning up after validating index on master])
[22:14:04,571][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] cluster state updated, version [8], source [create-index [authorities], cause [auto(index api)]]
[22:14:04,571][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] publishing cluster state version [8]
[22:14:04,571][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] set local cluster state to version 8
[22:14:04,571][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [[authorities/9lk82yMSTw6kl29yEW9PkQ]] creating index
[22:14:04,572][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating Index [[authorities/9lk82yMSTw6kl29yEW9PkQ]], shards [5]/[1] - reason [create index]
[22:14:04,572][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:05,037][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:05,038][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][1] creating shard
[22:14:05,038][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/9lk82yMSTw6kl29yEW9PkQ/1, shard=[authorities][1]}]
[22:14:05,038][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating shard_id [authorities][1]
[22:14:05,038][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:05,039][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:05,040][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:05,040][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][3] creating shard
[22:14:05,040][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#1]] starting recovery from store ...
[22:14:05,041][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/9lk82yMSTw6kl29yEW9PkQ/3, shard=[authorities][3]}]
[22:14:05,041][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating shard_id [authorities][3]
[22:14:05,041][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:05,041][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:05,042][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XF0WGbn][generic][T#1]] wipe translog location - creating new translog
[22:14:05,043][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:05,043][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][2] creating shard
[22:14:05,043][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#2]] starting recovery from store ...
[22:14:05,043][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/9lk82yMSTw6kl29yEW9PkQ/2, shard=[authorities][2]}]
[22:14:05,043][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating shard_id [authorities][2]
[22:14:05,043][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XF0WGbn][generic][T#1]] no translog ID present in the current generation - creating one
[22:14:05,044][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:05,044][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:05,045][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XF0WGbn][generic][T#2]] wipe translog location - creating new translog
[22:14:05,045][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:05,045][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][0] creating shard
[22:14:05,045][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#4]] starting recovery from store ...
[22:14:05,045][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/9lk82yMSTw6kl29yEW9PkQ/0, shard=[authorities][0]}]
[22:14:05,045][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating shard_id [authorities][0]
[22:14:05,045][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XF0WGbn][generic][T#2]] no translog ID present in the current generation - creating one
[22:14:05,046][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:05,046][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:05,046][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:05,046][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#1]] recovery completed from [shard_store], took [8ms]
[22:14:05,046][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#1]] [authorities][1] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[authorities][1]], allocation id [FrhFiM3kRcWTSJWvvmyxbw], primary term [0], message [after new shard recovery]]
[22:14:05,046][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#1]] [authorities][1] received shard started for [shard id [[authorities][1]], allocation id [FrhFiM3kRcWTSJWvvmyxbw], primary term [0], message [after new shard recovery]]
[22:14:05,046][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XF0WGbn][generic][T#4]] wipe translog location - creating new translog
[22:14:05,047][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:05,047][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#3]] starting recovery from store ...
[22:14:05,047][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:05,047][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#2]] recovery completed from [shard_store], took [6ms]
[22:14:05,047][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#2]] [authorities][3] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[authorities][3]], allocation id [voXa_RO8TU-1iADd7oR7aQ], primary term [0], message [after new shard recovery]]
[22:14:05,048][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#2]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [voXa_RO8TU-1iADd7oR7aQ], primary term [0], message [after new shard recovery]]
[22:14:05,048][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XF0WGbn][generic][T#4]] no translog ID present in the current generation - creating one
[22:14:05,048][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XF0WGbn][generic][T#3]] wipe translog location - creating new translog
[22:14:05,048][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: took [910ms] done applying updated cluster_state (version: 8, uuid: xU19J6qMRgCymNmu9WOO6w)
[22:14:05,049][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [FrhFiM3kRcWTSJWvvmyxbw], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [voXa_RO8TU-1iADd7oR7aQ], primary term [0], message [after new shard recovery]]]: execute
[22:14:05,049][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][1] starting shard [authorities][1], node[XF0WGbn-Qt-AlZKx5ywYQg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=FrhFiM3kRcWTSJWvvmyxbw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:04.570Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][1]], allocation id [FrhFiM3kRcWTSJWvvmyxbw], primary term [0], message [after new shard recovery]])
[22:14:05,049][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][3] starting shard [authorities][3], node[XF0WGbn-Qt-AlZKx5ywYQg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=voXa_RO8TU-1iADd7oR7aQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:04.570Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][3]], allocation id [voXa_RO8TU-1iADd7oR7aQ], primary term [0], message [after new shard recovery]])
[22:14:05,050][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XF0WGbn][generic][T#3]] no translog ID present in the current generation - creating one
[22:14:05,051][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:05,051][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#4]] recovery completed from [shard_store], took [7ms]
[22:14:05,051][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#4]] [authorities][2] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[authorities][2]], allocation id [BC1lTMcSRAGDYF4LBayOgQ], primary term [0], message [after new shard recovery]]
[22:14:05,051][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#4]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [BC1lTMcSRAGDYF4LBayOgQ], primary term [0], message [after new shard recovery]]
[22:14:05,052][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[authorities][1]], allocation id [FrhFiM3kRcWTSJWvvmyxbw], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [voXa_RO8TU-1iADd7oR7aQ], primary term [0], message [after new shard recovery]]]
[22:14:05,052][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] publishing cluster state version [9]
[22:14:05,052][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] set local cluster state to version 9
[22:14:05,053][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:05,053][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:05,053][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#3]] recovery completed from [shard_store], took [7ms]
[22:14:05,053][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#3]] [authorities][0] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[authorities][0]], allocation id [DF0QG6d0SwKxl0W-7kyusg], primary term [0], message [after new shard recovery]]
[22:14:05,053][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:05,054][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#3]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [DF0QG6d0SwKxl0W-7kyusg], primary term [0], message [after new shard recovery]]
[22:14:05,054][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][2] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[authorities][2]], allocation id [BC1lTMcSRAGDYF4LBayOgQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:05,054][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [BC1lTMcSRAGDYF4LBayOgQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:05,054][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][4] creating shard
[22:14:05,054][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/9lk82yMSTw6kl29yEW9PkQ/4, shard=[authorities][4]}]
[22:14:05,054][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] creating shard_id [authorities][4]
[22:14:05,056][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:05,056][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:05,057][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:05,057][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#1]] starting recovery from store ...
[22:14:05,057][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][0] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[authorities][0]], allocation id [DF0QG6d0SwKxl0W-7kyusg], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:05,057][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [DF0QG6d0SwKxl0W-7kyusg], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:05,058][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XF0WGbn][generic][T#1]] wipe translog location - creating new translog
[22:14:05,058][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [FrhFiM3kRcWTSJWvvmyxbw], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [voXa_RO8TU-1iADd7oR7aQ], primary term [0], message [after new shard recovery]]]: took [9ms] done applying updated cluster_state (version: 9, uuid: 6T-CBAuaQhKqtQGqSkVolg)
[22:14:05,059][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [BC1lTMcSRAGDYF4LBayOgQ], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [DF0QG6d0SwKxl0W-7kyusg], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [BC1lTMcSRAGDYF4LBayOgQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [DF0QG6d0SwKxl0W-7kyusg], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:05,059][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][2] starting shard [authorities][2], node[XF0WGbn-Qt-AlZKx5ywYQg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=BC1lTMcSRAGDYF4LBayOgQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:04.570Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][2]], allocation id [BC1lTMcSRAGDYF4LBayOgQ], primary term [0], message [after new shard recovery]])
[22:14:05,059][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XF0WGbn][generic][T#1]] no translog ID present in the current generation - creating one
[22:14:05,060][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][0] starting shard [authorities][0], node[XF0WGbn-Qt-AlZKx5ywYQg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=DF0QG6d0SwKxl0W-7kyusg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:04.570Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][0]], allocation id [DF0QG6d0SwKxl0W-7kyusg], primary term [0], message [after new shard recovery]])
[22:14:05,062][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[authorities][2]], allocation id [BC1lTMcSRAGDYF4LBayOgQ], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [DF0QG6d0SwKxl0W-7kyusg], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [BC1lTMcSRAGDYF4LBayOgQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [DF0QG6d0SwKxl0W-7kyusg], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:05,062][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] publishing cluster state version [10]
[22:14:05,062][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] set local cluster state to version 10
[22:14:05,063][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:05,063][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][generic][T#1]] recovery completed from [shard_store], took [8ms]
[22:14:05,063][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:05,063][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[authorities][4]], allocation id [aQ3--az4RfmTOuTTlaOKwQ], primary term [0], message [after new shard recovery]]
[22:14:05,064][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][generic][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [aQ3--az4RfmTOuTTlaOKwQ], primary term [0], message [after new shard recovery]]
[22:14:05,064][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [XF0WGbn-Qt-AlZKx5ywYQg] for shard entry [shard id [[authorities][4]], allocation id [aQ3--az4RfmTOuTTlaOKwQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:05,064][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [aQ3--az4RfmTOuTTlaOKwQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:05,064][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:05,066][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [BC1lTMcSRAGDYF4LBayOgQ], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [DF0QG6d0SwKxl0W-7kyusg], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [BC1lTMcSRAGDYF4LBayOgQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [DF0QG6d0SwKxl0W-7kyusg], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [6ms] done applying updated cluster_state (version: 10, uuid: 5n0wen50TnCSHSl3NHF-zQ)
[22:14:05,066][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [aQ3--az4RfmTOuTTlaOKwQ], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [aQ3--az4RfmTOuTTlaOKwQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:05,066][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities][4] starting shard [authorities][4], node[XF0WGbn-Qt-AlZKx5ywYQg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=aQ3--az4RfmTOuTTlaOKwQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:04.570Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[authorities][4]], allocation id [aQ3--az4RfmTOuTTlaOKwQ], primary term [0], message [after new shard recovery]])
[22:14:05,067][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] cluster state updated, version [11], source [shard-started[shard id [[authorities][4]], allocation id [aQ3--az4RfmTOuTTlaOKwQ], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [aQ3--az4RfmTOuTTlaOKwQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:05,067][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] publishing cluster state version [11]
[22:14:05,068][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] set local cluster state to version 11
[22:14:05,068][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:05,070][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [aQ3--az4RfmTOuTTlaOKwQ], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [aQ3--az4RfmTOuTTlaOKwQ], primary term [0], message [master {XF0WGbn}{XF0WGbn-Qt-AlZKx5ywYQg}{9rnfrr5iScqz9aLQNpAm4w}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [3ms] done applying updated cluster_state (version: 11, uuid: oEt4K47XToW9OYFEHySH_A)
[22:14:05,071][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: execute
[22:14:05,520][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:05,522][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [authorities/9lk82yMSTw6kl29yEW9PkQ] create_mapping [persons] with source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:05,522][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] cluster state updated, version [12], source [put-mapping[persons]]
[22:14:05,523][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] publishing cluster state version [12]
[22:14:05,523][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] set local cluster state to version 12
[22:14:05,523][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] [[authorities/9lk82yMSTw6kl29yEW9PkQ]] adding mapping [persons], source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:05,527][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XF0WGbn][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: took [454ms] done applying updated cluster_state (version: 12, uuid: bOvsFgWUQ8uOwCKuU2pi1A)
[22:14:06,002][DEBUG][org.elasticsearch.index.mapper.MapperService][Test worker] using dynamic[true]
[22:14:06,005][INFO ][test                     ][Test worker] stopping nodes
[22:14:06,005][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[22:14:06,006][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test] closing ... (reason [shutdown])
[22:14:06,006][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/iQ_ETgOoSTSwD63U_ZbfQA] closing index service (reason [shutdown])
[22:14:06,006][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[22:14:06,006][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities] closing ... (reason [shutdown])
[22:14:06,007][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:06,007][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/9lk82yMSTw6kl29yEW9PkQ] closing index service (reason [shutdown])
[22:14:06,007][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:06,007][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[22:14:06,007][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:06,007][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:06,007][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:06,007][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:06,007][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:06,007][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:06,007][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:06,007][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:06,008][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:06,008][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:06,008][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:06,009][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[22:14:06,009][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:06,009][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[22:14:06,009][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[22:14:06,009][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[22:14:06,009][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:06,009][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:06,009][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:06,009][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:06,009][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:06,009][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:06,009][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:06,009][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:06,010][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:06,010][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:06,011][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:06,011][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:06,011][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:06,011][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:06,011][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[22:14:06,011][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[22:14:06,011][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[22:14:06,011][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[22:14:06,011][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:06,011][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:06,011][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:06,011][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:06,011][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:06,011][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:06,011][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:06,011][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:06,012][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:06,012][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:06,012][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:06,012][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:06,012][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:06,012][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:06,013][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[22:14:06,013][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[22:14:06,013][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[22:14:06,013][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[22:14:06,013][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:06,013][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:06,013][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:06,013][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:06,013][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:06,013][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:06,013][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:06,014][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:06,014][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:06,014][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[22:14:06,014][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[22:14:06,014][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:06,014][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:06,019][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:06,019][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:06,019][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:06,020][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:06,020][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:06,020][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:06,020][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:06,021][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:06,021][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[22:14:06,021][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:14:06,021][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:06,021][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[22:14:06,021][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:06,021][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:14:06,021][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[22:14:06,021][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[22:14:06,021][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:06,021][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/iQ_ETgOoSTSwD63U_ZbfQA] closed... (reason [shutdown])
[22:14:06,021][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:06,021][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:06,021][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:06,022][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:06,023][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:06,023][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:06,023][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[22:14:06,023][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[22:14:06,023][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[22:14:06,023][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[22:14:06,023][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/9lk82yMSTw6kl29yEW9PkQ] closed... (reason [shutdown])
[22:14:06,023][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[22:14:06,023][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[22:14:06,024][INFO ][org.elasticsearch.node.Node][Test worker] closed
[22:14:06,031][INFO ][test                     ][Test worker] data files wiped
[22:14:08,032][INFO ][test                     ][Test worker] settings cluster name
[22:14:08,032][INFO ][test                     ][Test worker] starting nodes
[22:14:08,032][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[22:14:08,034][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[22:14:08,055][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[22:14:08,056][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [138.5gb], usable_space [138.3gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[22:14:08,056][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[22:14:08,056][INFO ][org.elasticsearch.node.Node][Test worker] node name [fNip1nv] derived from node ID [fNip1nv0Th2UFHkebrQhxg]; set [node.name] to override
[22:14:08,056][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[5199], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[22:14:08,056][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[22:14:08,056][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[22:14:08,057][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[22:14:08,057][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[22:14:08,057][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[22:14:08,057][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[22:14:08,057][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[22:14:08,057][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[22:14:08,057][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[22:14:08,057][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[22:14:08,058][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[22:14:08,058][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[22:14:08,058][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[22:14:08,058][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[22:14:08,058][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[22:14:08,058][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[22:14:08,058][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[22:14:08,058][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[22:14:08,059][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[22:14:08,063][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:45e7:7aff:1af8:a30a prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[22:14:08,063][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[22:14:08,063][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[22:14:08,063][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[22:14:08,063][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[22:14:08,063][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[22:14:08,064][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[22:14:08,064][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[22:14:08,064][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[22:14:08,065][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[22:14:08,065][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[22:14:08,065][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[22:14:08,065][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[22:14:08,065][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[22:14:08,066][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[22:14:08,066][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[22:14:08,066][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:14:08,066][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:14:08,091][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[22:14:08,095][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[22:14:08,276][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[22:14:08,280][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[22:14:08,280][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[22:14:08,280][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[22:14:08,281][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[13]}, bound_addresses {local[13]}
[22:14:08,281][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[22:14:08,282][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [initial_join]: execute
[22:14:08,282][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[22:14:11,286][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[fNip1nv][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]}], id[91], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[22:14:11,288][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[fNip1nv][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[22:14:11,289][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[22:14:11,290][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[22:14:11,290][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] new_master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[22:14:11,290][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] publishing cluster state version [1]
[22:14:11,290][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] set local cluster state to version 1
[22:14:11,291][INFO ][org.elasticsearch.node.Node][Test worker] started
[22:14:11,291][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [2ms] done applying updated cluster_state (version: 1, uuid: 0YlzZG5mRX-aQFaVtS1NsA)
[22:14:11,292][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[22:14:11,292][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[22:14:11,292][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] publishing cluster state version [2]
[22:14:11,292][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] set local cluster state to version 2
[22:14:11,294][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[22:14:11,294][INFO ][test                     ][Test worker] nodes are started
[22:14:11,294][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [2ms] done applying updated cluster_state (version: 2, uuid: mjeHDk3oSCeOJVcmBvdNTw)
[22:14:11,295][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'test' index
[22:14:11,295][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: execute
[22:14:11,295][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating Index [[test/Aqe1zaiYRtWjwpWfNYvisQ]], shards [5]/[1] - reason [create index]
[22:14:11,296][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:11,753][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:11,754][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[22:14:11,755][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[22:14:11,755][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test/Aqe1zaiYRtWjwpWfNYvisQ] closing index service (reason [cleaning up after validating index on master])
[22:14:11,755][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:11,755][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] full cache clear, reason [close]
[22:14:11,755][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:11,755][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test/Aqe1zaiYRtWjwpWfNYvisQ] closed... (reason [cleaning up after validating index on master])
[22:14:11,755][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [auto(index api)]]
[22:14:11,755][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] publishing cluster state version [3]
[22:14:11,755][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] set local cluster state to version 3
[22:14:11,756][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [[test/Aqe1zaiYRtWjwpWfNYvisQ]] creating index
[22:14:11,756][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating Index [[test/Aqe1zaiYRtWjwpWfNYvisQ]], shards [5]/[1] - reason [create index]
[22:14:11,756][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:12,198][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:12,198][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][1] creating shard
[22:14:12,198][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Aqe1zaiYRtWjwpWfNYvisQ/1, shard=[test][1]}]
[22:14:12,198][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating shard_id [test][1]
[22:14:12,199][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:12,199][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:12,200][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:12,200][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][3] creating shard
[22:14:12,200][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#3]] starting recovery from store ...
[22:14:12,200][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Aqe1zaiYRtWjwpWfNYvisQ/3, shard=[test][3]}]
[22:14:12,200][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating shard_id [test][3]
[22:14:12,201][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:12,201][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:12,201][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[fNip1nv][generic][T#3]] wipe translog location - creating new translog
[22:14:12,202][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:12,202][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][2] creating shard
[22:14:12,202][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#1]] starting recovery from store ...
[22:14:12,202][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Aqe1zaiYRtWjwpWfNYvisQ/2, shard=[test][2]}]
[22:14:12,202][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating shard_id [test][2]
[22:14:12,202][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[fNip1nv][generic][T#3]] no translog ID present in the current generation - creating one
[22:14:12,203][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:12,203][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:12,203][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[fNip1nv][generic][T#1]] wipe translog location - creating new translog
[22:14:12,204][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:12,204][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][0] creating shard
[22:14:12,204][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#2]] starting recovery from store ...
[22:14:12,204][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Aqe1zaiYRtWjwpWfNYvisQ/0, shard=[test][0]}]
[22:14:12,204][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating shard_id [test][0]
[22:14:12,204][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[fNip1nv][generic][T#1]] no translog ID present in the current generation - creating one
[22:14:12,205][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:12,205][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:12,205][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:12,205][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#3]] recovery completed from [shard_store], took [6ms]
[22:14:12,205][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[test][1]], allocation id [SabKwZssQ6mAwXISYu7WBg], primary term [0], message [after new shard recovery]]
[22:14:12,205][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [SabKwZssQ6mAwXISYu7WBg], primary term [0], message [after new shard recovery]]
[22:14:12,205][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[fNip1nv][generic][T#2]] wipe translog location - creating new translog
[22:14:12,206][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:12,206][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#4]] starting recovery from store ...
[22:14:12,206][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:12,206][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#1]] recovery completed from [shard_store], took [6ms]
[22:14:12,206][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#1]] [test][3] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[test][3]], allocation id [bm9djB98QNqwEMKphwj8gg], primary term [0], message [after new shard recovery]]
[22:14:12,206][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [bm9djB98QNqwEMKphwj8gg], primary term [0], message [after new shard recovery]]
[22:14:12,206][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[fNip1nv][generic][T#2]] no translog ID present in the current generation - creating one
[22:14:12,207][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: took [911ms] done applying updated cluster_state (version: 3, uuid: Hfk0fVX4TQaWZE5P2fzgKQ)
[22:14:12,207][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [SabKwZssQ6mAwXISYu7WBg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [bm9djB98QNqwEMKphwj8gg], primary term [0], message [after new shard recovery]]]: execute
[22:14:12,207][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[fNip1nv][generic][T#4]] wipe translog location - creating new translog
[22:14:12,207][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[fNip1nv0Th2UFHkebrQhxg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=SabKwZssQ6mAwXISYu7WBg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:11.754Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [SabKwZssQ6mAwXISYu7WBg], primary term [0], message [after new shard recovery]])
[22:14:12,207][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[fNip1nv0Th2UFHkebrQhxg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=bm9djB98QNqwEMKphwj8gg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:11.754Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [bm9djB98QNqwEMKphwj8gg], primary term [0], message [after new shard recovery]])
[22:14:12,209][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [SabKwZssQ6mAwXISYu7WBg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [bm9djB98QNqwEMKphwj8gg], primary term [0], message [after new shard recovery]]]
[22:14:12,209][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] publishing cluster state version [4]
[22:14:12,209][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] set local cluster state to version 4
[22:14:12,209][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[fNip1nv][generic][T#4]] no translog ID present in the current generation - creating one
[22:14:12,211][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:12,211][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#2]] recovery completed from [shard_store], took [8ms]
[22:14:12,211][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#2]] [test][2] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[test][2]], allocation id [x7tq8ZgGTuW6D-M7xbTpcg], primary term [0], message [after new shard recovery]]
[22:14:12,211][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#2]] [test][2] received shard started for [shard id [[test][2]], allocation id [x7tq8ZgGTuW6D-M7xbTpcg], primary term [0], message [after new shard recovery]]
[22:14:12,213][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:12,213][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#4]] recovery completed from [shard_store], took [9ms]
[22:14:12,214][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:12,214][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[test][0]], allocation id [slELPkBGSDWKyOfMkeGyRA], primary term [0], message [after new shard recovery]]
[22:14:12,214][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [slELPkBGSDWKyOfMkeGyRA], primary term [0], message [after new shard recovery]]
[22:14:12,214][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:12,214][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][2] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[test][2]], allocation id [x7tq8ZgGTuW6D-M7xbTpcg], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:12,215][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [x7tq8ZgGTuW6D-M7xbTpcg], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:12,215][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][4] creating shard
[22:14:12,215][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Aqe1zaiYRtWjwpWfNYvisQ/4, shard=[test][4]}]
[22:14:12,215][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating shard_id [test][4]
[22:14:12,217][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:12,217][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:12,219][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:12,220][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#3]] starting recovery from store ...
[22:14:12,220][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[test][0]], allocation id [slELPkBGSDWKyOfMkeGyRA], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:12,220][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [slELPkBGSDWKyOfMkeGyRA], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:12,221][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[fNip1nv][generic][T#3]] wipe translog location - creating new translog
[22:14:12,222][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [SabKwZssQ6mAwXISYu7WBg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [bm9djB98QNqwEMKphwj8gg], primary term [0], message [after new shard recovery]]]: took [14ms] done applying updated cluster_state (version: 4, uuid: PMKL-bXrSQ-km07N_9HkSQ)
[22:14:12,222][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [x7tq8ZgGTuW6D-M7xbTpcg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [slELPkBGSDWKyOfMkeGyRA], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [x7tq8ZgGTuW6D-M7xbTpcg], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [slELPkBGSDWKyOfMkeGyRA], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:12,222][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[fNip1nv0Th2UFHkebrQhxg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=x7tq8ZgGTuW6D-M7xbTpcg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:11.754Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [x7tq8ZgGTuW6D-M7xbTpcg], primary term [0], message [after new shard recovery]])
[22:14:12,222][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[fNip1nv0Th2UFHkebrQhxg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=slELPkBGSDWKyOfMkeGyRA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:11.754Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [slELPkBGSDWKyOfMkeGyRA], primary term [0], message [after new shard recovery]])
[22:14:12,222][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[fNip1nv][generic][T#3]] no translog ID present in the current generation - creating one
[22:14:12,223][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][2]], allocation id [x7tq8ZgGTuW6D-M7xbTpcg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [slELPkBGSDWKyOfMkeGyRA], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [x7tq8ZgGTuW6D-M7xbTpcg], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [slELPkBGSDWKyOfMkeGyRA], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:12,224][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] publishing cluster state version [5]
[22:14:12,224][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] set local cluster state to version 5
[22:14:12,226][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:12,226][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#3]] recovery completed from [shard_store], took [10ms]
[22:14:12,226][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[test][4]], allocation id [-qbGNwj6S4ayk7Ca-Grn2Q], primary term [0], message [after new shard recovery]]
[22:14:12,226][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [-qbGNwj6S4ayk7Ca-Grn2Q], primary term [0], message [after new shard recovery]]
[22:14:12,227][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:12,227][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[test][4]], allocation id [-qbGNwj6S4ayk7Ca-Grn2Q], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:12,227][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [-qbGNwj6S4ayk7Ca-Grn2Q], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:12,227][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:12,230][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [x7tq8ZgGTuW6D-M7xbTpcg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [slELPkBGSDWKyOfMkeGyRA], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [x7tq8ZgGTuW6D-M7xbTpcg], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [slELPkBGSDWKyOfMkeGyRA], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [7ms] done applying updated cluster_state (version: 5, uuid: il95mj06RWaF66lZpm14cg)
[22:14:12,230][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [-qbGNwj6S4ayk7Ca-Grn2Q], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [-qbGNwj6S4ayk7Ca-Grn2Q], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:12,230][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[fNip1nv0Th2UFHkebrQhxg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=-qbGNwj6S4ayk7Ca-Grn2Q], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:11.754Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [-qbGNwj6S4ayk7Ca-Grn2Q], primary term [0], message [after new shard recovery]])
[22:14:12,231][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [-qbGNwj6S4ayk7Ca-Grn2Q], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [-qbGNwj6S4ayk7Ca-Grn2Q], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:12,231][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] publishing cluster state version [6]
[22:14:12,232][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] set local cluster state to version 6
[22:14:12,232][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:12,233][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [-qbGNwj6S4ayk7Ca-Grn2Q], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [-qbGNwj6S4ayk7Ca-Grn2Q], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 6, uuid: l9DEvALLTrOBT5xyLtEMJg)
[22:14:12,235][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[22:14:12,680][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:12,681][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [test/Aqe1zaiYRtWjwpWfNYvisQ] create_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:12,682][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] cluster state updated, version [7], source [put-mapping[test]]
[22:14:12,682][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] publishing cluster state version [7]
[22:14:12,682][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] set local cluster state to version 7
[22:14:12,682][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [[test/Aqe1zaiYRtWjwpWfNYvisQ]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:12,684][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [449ms] done applying updated cluster_state (version: 7, uuid: K2PvIQrVSUGdNzDNj9DJkw)
[22:14:12,716][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'authorities' index
[22:14:12,717][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: execute
[22:14:12,717][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating Index [[authorities/3BtWjU0PS3GhL7IFIYrgsw]], shards [5]/[1] - reason [create index]
[22:14:12,717][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:13,157][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:13,158][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[22:14:13,159][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities] closing ... (reason [cleaning up after validating index on master])
[22:14:13,159][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities/3BtWjU0PS3GhL7IFIYrgsw] closing index service (reason [cleaning up after validating index on master])
[22:14:13,159][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:13,159][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] full cache clear, reason [close]
[22:14:13,159][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:13,159][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities/3BtWjU0PS3GhL7IFIYrgsw] closed... (reason [cleaning up after validating index on master])
[22:14:13,159][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] cluster state updated, version [8], source [create-index [authorities], cause [auto(index api)]]
[22:14:13,159][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] publishing cluster state version [8]
[22:14:13,159][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] set local cluster state to version 8
[22:14:13,160][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [[authorities/3BtWjU0PS3GhL7IFIYrgsw]] creating index
[22:14:13,160][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating Index [[authorities/3BtWjU0PS3GhL7IFIYrgsw]], shards [5]/[1] - reason [create index]
[22:14:13,160][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:13,639][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:13,639][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][1] creating shard
[22:14:13,639][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/3BtWjU0PS3GhL7IFIYrgsw/1, shard=[authorities][1]}]
[22:14:13,639][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating shard_id [authorities][1]
[22:14:13,640][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:13,640][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:13,640][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:13,641][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][3] creating shard
[22:14:13,641][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#1]] starting recovery from store ...
[22:14:13,641][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/3BtWjU0PS3GhL7IFIYrgsw/3, shard=[authorities][3]}]
[22:14:13,641][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating shard_id [authorities][3]
[22:14:13,641][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:13,641][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:13,641][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[fNip1nv][generic][T#1]] wipe translog location - creating new translog
[22:14:13,642][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:13,642][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][2] creating shard
[22:14:13,642][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#2]] starting recovery from store ...
[22:14:13,642][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/3BtWjU0PS3GhL7IFIYrgsw/2, shard=[authorities][2]}]
[22:14:13,642][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating shard_id [authorities][2]
[22:14:13,642][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[fNip1nv][generic][T#1]] no translog ID present in the current generation - creating one
[22:14:13,643][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:13,643][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:13,643][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[fNip1nv][generic][T#2]] wipe translog location - creating new translog
[22:14:13,644][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:13,644][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][0] creating shard
[22:14:13,644][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[fNip1nv][generic][T#2]] no translog ID present in the current generation - creating one
[22:14:13,644][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#4]] starting recovery from store ...
[22:14:13,644][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/3BtWjU0PS3GhL7IFIYrgsw/0, shard=[authorities][0]}]
[22:14:13,644][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating shard_id [authorities][0]
[22:14:13,645][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:13,646][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:13,646][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#1]] recovery completed from [shard_store], took [6ms]
[22:14:13,646][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#1]] [authorities][1] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[authorities][1]], allocation id [XRpifsgLTECgkUOkSXfI-w], primary term [0], message [after new shard recovery]]
[22:14:13,646][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#1]] [authorities][1] received shard started for [shard id [[authorities][1]], allocation id [XRpifsgLTECgkUOkSXfI-w], primary term [0], message [after new shard recovery]]
[22:14:13,646][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:13,646][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[fNip1nv][generic][T#4]] wipe translog location - creating new translog
[22:14:13,647][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:13,648][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#2]] recovery completed from [shard_store], took [6ms]
[22:14:13,648][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:13,648][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#2]] [authorities][3] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[authorities][3]], allocation id [ijurLvMmRCmdxOpHsAQnlQ], primary term [0], message [after new shard recovery]]
[22:14:13,648][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#2]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [ijurLvMmRCmdxOpHsAQnlQ], primary term [0], message [after new shard recovery]]
[22:14:13,648][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[fNip1nv][generic][T#4]] no translog ID present in the current generation - creating one
[22:14:13,648][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#3]] starting recovery from store ...
[22:14:13,649][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[fNip1nv][generic][T#3]] wipe translog location - creating new translog
[22:14:13,649][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: took [932ms] done applying updated cluster_state (version: 8, uuid: YdC131utRLyjZmqCzr4yNg)
[22:14:13,650][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [XRpifsgLTECgkUOkSXfI-w], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [ijurLvMmRCmdxOpHsAQnlQ], primary term [0], message [after new shard recovery]]]: execute
[22:14:13,650][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][1] starting shard [authorities][1], node[fNip1nv0Th2UFHkebrQhxg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=XRpifsgLTECgkUOkSXfI-w], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:13.158Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][1]], allocation id [XRpifsgLTECgkUOkSXfI-w], primary term [0], message [after new shard recovery]])
[22:14:13,650][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:13,650][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][3] starting shard [authorities][3], node[fNip1nv0Th2UFHkebrQhxg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=ijurLvMmRCmdxOpHsAQnlQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:13.158Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][3]], allocation id [ijurLvMmRCmdxOpHsAQnlQ], primary term [0], message [after new shard recovery]])
[22:14:13,650][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#4]] recovery completed from [shard_store], took [7ms]
[22:14:13,650][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#4]] [authorities][2] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[authorities][2]], allocation id [MjyIu6LoSz2brVqSvWsxpw], primary term [0], message [after new shard recovery]]
[22:14:13,650][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#4]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [MjyIu6LoSz2brVqSvWsxpw], primary term [0], message [after new shard recovery]]
[22:14:13,650][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[fNip1nv][generic][T#3]] no translog ID present in the current generation - creating one
[22:14:13,651][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[authorities][1]], allocation id [XRpifsgLTECgkUOkSXfI-w], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [ijurLvMmRCmdxOpHsAQnlQ], primary term [0], message [after new shard recovery]]]
[22:14:13,652][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] publishing cluster state version [9]
[22:14:13,652][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] set local cluster state to version 9
[22:14:13,653][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:13,653][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:13,653][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#3]] recovery completed from [shard_store], took [8ms]
[22:14:13,653][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#3]] [authorities][0] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[authorities][0]], allocation id [CL0CJN7BTrOdS2gBhkCqlg], primary term [0], message [after new shard recovery]]
[22:14:13,653][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:13,653][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#3]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [CL0CJN7BTrOdS2gBhkCqlg], primary term [0], message [after new shard recovery]]
[22:14:13,653][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][2] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[authorities][2]], allocation id [MjyIu6LoSz2brVqSvWsxpw], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:13,653][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [MjyIu6LoSz2brVqSvWsxpw], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:13,654][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][4] creating shard
[22:14:13,654][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/3BtWjU0PS3GhL7IFIYrgsw/4, shard=[authorities][4]}]
[22:14:13,654][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] creating shard_id [authorities][4]
[22:14:13,655][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:13,655][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:13,656][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:13,656][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#1]] starting recovery from store ...
[22:14:13,656][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][0] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[authorities][0]], allocation id [CL0CJN7BTrOdS2gBhkCqlg], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:13,657][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [CL0CJN7BTrOdS2gBhkCqlg], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:13,657][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[fNip1nv][generic][T#1]] wipe translog location - creating new translog
[22:14:13,658][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [XRpifsgLTECgkUOkSXfI-w], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [ijurLvMmRCmdxOpHsAQnlQ], primary term [0], message [after new shard recovery]]]: took [8ms] done applying updated cluster_state (version: 9, uuid: 3sDRHBLFR4W3YuqyhaIXQA)
[22:14:13,658][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [MjyIu6LoSz2brVqSvWsxpw], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [CL0CJN7BTrOdS2gBhkCqlg], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [MjyIu6LoSz2brVqSvWsxpw], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [CL0CJN7BTrOdS2gBhkCqlg], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:13,658][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][2] starting shard [authorities][2], node[fNip1nv0Th2UFHkebrQhxg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=MjyIu6LoSz2brVqSvWsxpw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:13.158Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][2]], allocation id [MjyIu6LoSz2brVqSvWsxpw], primary term [0], message [after new shard recovery]])
[22:14:13,658][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[fNip1nv][generic][T#1]] no translog ID present in the current generation - creating one
[22:14:13,658][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][0] starting shard [authorities][0], node[fNip1nv0Th2UFHkebrQhxg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=CL0CJN7BTrOdS2gBhkCqlg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:13.158Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][0]], allocation id [CL0CJN7BTrOdS2gBhkCqlg], primary term [0], message [after new shard recovery]])
[22:14:13,659][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[authorities][2]], allocation id [MjyIu6LoSz2brVqSvWsxpw], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [CL0CJN7BTrOdS2gBhkCqlg], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [MjyIu6LoSz2brVqSvWsxpw], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [CL0CJN7BTrOdS2gBhkCqlg], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:13,659][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:13,659][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] publishing cluster state version [10]
[22:14:13,659][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][generic][T#1]] recovery completed from [shard_store], took [5ms]
[22:14:13,660][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[authorities][4]], allocation id [cx-kNwsjRiGjtYuW0Gptag], primary term [0], message [after new shard recovery]]
[22:14:13,660][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] set local cluster state to version 10
[22:14:13,660][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][generic][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [cx-kNwsjRiGjtYuW0Gptag], primary term [0], message [after new shard recovery]]
[22:14:13,660][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:13,661][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [fNip1nv0Th2UFHkebrQhxg] for shard entry [shard id [[authorities][4]], allocation id [cx-kNwsjRiGjtYuW0Gptag], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:13,661][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [cx-kNwsjRiGjtYuW0Gptag], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:13,661][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:13,662][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [MjyIu6LoSz2brVqSvWsxpw], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [CL0CJN7BTrOdS2gBhkCqlg], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [MjyIu6LoSz2brVqSvWsxpw], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [CL0CJN7BTrOdS2gBhkCqlg], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 10, uuid: N7w-1N5sQHCHt5O1Xvm3-g)
[22:14:13,662][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [cx-kNwsjRiGjtYuW0Gptag], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [cx-kNwsjRiGjtYuW0Gptag], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:13,662][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities][4] starting shard [authorities][4], node[fNip1nv0Th2UFHkebrQhxg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=cx-kNwsjRiGjtYuW0Gptag], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:13.158Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[authorities][4]], allocation id [cx-kNwsjRiGjtYuW0Gptag], primary term [0], message [after new shard recovery]])
[22:14:13,663][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] cluster state updated, version [11], source [shard-started[shard id [[authorities][4]], allocation id [cx-kNwsjRiGjtYuW0Gptag], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [cx-kNwsjRiGjtYuW0Gptag], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:13,663][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] publishing cluster state version [11]
[22:14:13,664][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] set local cluster state to version 11
[22:14:13,664][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:13,665][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [cx-kNwsjRiGjtYuW0Gptag], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [cx-kNwsjRiGjtYuW0Gptag], primary term [0], message [master {fNip1nv}{fNip1nv0Th2UFHkebrQhxg}{CpEl1Bu2S3C8VbV0WAV48w}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 11, uuid: KRNjV24oRk-usT9qviCSuQ)
[22:14:13,668][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: execute
[22:14:14,117][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:14,119][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [authorities/3BtWjU0PS3GhL7IFIYrgsw] create_mapping [persons] with source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:14,119][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] cluster state updated, version [12], source [put-mapping[persons]]
[22:14:14,119][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] publishing cluster state version [12]
[22:14:14,119][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] set local cluster state to version 12
[22:14:14,119][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] [[authorities/3BtWjU0PS3GhL7IFIYrgsw]] adding mapping [persons], source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:14,122][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[fNip1nv][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: took [453ms] done applying updated cluster_state (version: 12, uuid: xBJAi_ybTRCyDoMHpqCv4g)
[22:14:14,613][DEBUG][org.elasticsearch.index.mapper.MapperService][Test worker] using dynamic[true]
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _source = null
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _type = docs
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _type = null
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _uid = docs#1
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _version = -1
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc title = A title
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc title.keyword = null
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc title.keyword = null
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc dc.creator = A creator
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc bib.contributor = A contributor
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc authorID = 1
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc dc.creator = John Doe
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc bib.contributor = John Doe
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _all = A title
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _all = A creator
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _all = A contributor
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _all = 1
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _source
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _type
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _type
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _uid
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _version
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title.keyword
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title.keyword
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = dc
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = dc.creator
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = bib
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = bib.contributor
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = authorID
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = dc
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = dc.creator
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = bib
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = bib.contributor
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _all
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _all
[22:14:14,615][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _all
[22:14:14,616][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _all
[22:14:14,616][INFO ][test                     ][Test worker] stopping nodes
[22:14:14,616][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[22:14:14,616][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test] closing ... (reason [shutdown])
[22:14:14,617][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/Aqe1zaiYRtWjwpWfNYvisQ] closing index service (reason [shutdown])
[22:14:14,617][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[22:14:14,617][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities] closing ... (reason [shutdown])
[22:14:14,617][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:14,617][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/3BtWjU0PS3GhL7IFIYrgsw] closing index service (reason [shutdown])
[22:14:14,617][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:14,617][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[22:14:14,617][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:14,617][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:14,617][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:14,618][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:14,618][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:14,618][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:14,618][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:14,618][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:14,618][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:14,618][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:14,618][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:14,618][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:14,618][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[22:14:14,618][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[22:14:14,618][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[22:14:14,618][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[22:14:14,618][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:14,618][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:14,618][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:14,618][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:14,618][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:14,618][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:14,618][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:14,618][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:14,618][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:14,618][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:14,619][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:14,619][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:14,619][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:14,619][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[22:14:14,619][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:14,619][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[22:14:14,619][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[22:14:14,619][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[22:14:14,619][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:14,619][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:14,619][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:14,619][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:14,619][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:14,619][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:14,619][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:14,619][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:14,619][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:14,619][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:14,619][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:14,619][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:14,619][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:14,619][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:14,619][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[22:14:14,619][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[22:14:14,619][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[22:14:14,619][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[22:14:14,619][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:14,619][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:14,619][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:14,619][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:14,620][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:14,620][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:14,620][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:14,620][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:14,620][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:14,620][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[22:14:14,620][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[22:14:14,620][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:14,620][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:14,624][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:14,625][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:14,625][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:14,625][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:14,625][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:14,625][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[22:14:14,625][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:14:14,625][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:14,625][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[22:14:14,625][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:14,625][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:14:14,625][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:14,626][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/Aqe1zaiYRtWjwpWfNYvisQ] closed... (reason [shutdown])
[22:14:14,626][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:14,626][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:14,626][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[22:14:14,626][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[22:14:14,626][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:14,626][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:14,626][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:14,626][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:14,626][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:14,627][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:14,627][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:14,627][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[22:14:14,627][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[22:14:14,627][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[22:14:14,627][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[22:14:14,628][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/3BtWjU0PS3GhL7IFIYrgsw] closed... (reason [shutdown])
[22:14:14,628][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[22:14:14,628][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[22:14:14,630][INFO ][org.elasticsearch.node.Node][Test worker] closed
[22:14:14,638][INFO ][test                     ][Test worker] data files wiped
[22:14:16,639][INFO ][test                     ][Test worker] settings cluster name
[22:14:16,639][INFO ][test                     ][Test worker] starting nodes
[22:14:16,639][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[22:14:16,642][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[22:14:16,653][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[22:14:16,653][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [138.5gb], usable_space [138.3gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[22:14:16,653][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[22:14:16,654][INFO ][org.elasticsearch.node.Node][Test worker] node name [nokQigQ] derived from node ID [nokQigQlTymAHf0u0raGbQ]; set [node.name] to override
[22:14:16,654][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[5199], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[22:14:16,654][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[22:14:16,654][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[22:14:16,654][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[22:14:16,654][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[22:14:16,655][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[22:14:16,656][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[22:14:16,656][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[22:14:16,660][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:45e7:7aff:1af8:a30a prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[22:14:16,661][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[22:14:16,661][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[22:14:16,662][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[22:14:16,662][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[22:14:16,662][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[22:14:16,662][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[22:14:16,663][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[22:14:16,663][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[22:14:16,664][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[22:14:16,664][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[22:14:16,664][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[22:14:16,665][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[22:14:16,665][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[22:14:16,665][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[22:14:16,665][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[22:14:16,665][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:14:16,665][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:14:16,690][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[22:14:16,694][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[22:14:16,881][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[22:14:16,885][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[22:14:16,885][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[22:14:16,885][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[22:14:16,886][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[14]}, bound_addresses {local[14]}
[22:14:16,886][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[22:14:16,886][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [initial_join]: execute
[22:14:16,887][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[22:14:19,893][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[nokQigQ][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]}], id[98], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[22:14:19,893][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[nokQigQ][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[22:14:19,894][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[22:14:19,894][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[22:14:19,894][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] new_master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[22:14:19,894][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [1]
[22:14:19,894][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 1
[22:14:19,895][INFO ][org.elasticsearch.node.Node][Test worker] started
[22:14:19,895][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [0s] done applying updated cluster_state (version: 1, uuid: 2GsHOFpsSDeI3KG2uywiaw)
[22:14:19,895][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[22:14:19,895][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[22:14:19,895][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [2]
[22:14:19,895][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 2
[22:14:19,897][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[22:14:19,897][INFO ][test                     ][Test worker] nodes are started
[22:14:19,898][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [2ms] done applying updated cluster_state (version: 2, uuid: KR83RpYXRW2UotbNfqqkfw)
[22:14:19,898][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'test' index
[22:14:19,898][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: execute
[22:14:19,898][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating Index [[test/R0SQI1RjTl-3KhyeOxR1ag]], shards [5]/[1] - reason [create index]
[22:14:19,899][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:20,367][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:20,367][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[22:14:20,368][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[22:14:20,368][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test/R0SQI1RjTl-3KhyeOxR1ag] closing index service (reason [cleaning up after validating index on master])
[22:14:20,368][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:20,368][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] full cache clear, reason [close]
[22:14:20,368][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:20,368][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test/R0SQI1RjTl-3KhyeOxR1ag] closed... (reason [cleaning up after validating index on master])
[22:14:20,369][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [auto(index api)]]
[22:14:20,369][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [3]
[22:14:20,369][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 3
[22:14:20,369][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [[test/R0SQI1RjTl-3KhyeOxR1ag]] creating index
[22:14:20,369][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating Index [[test/R0SQI1RjTl-3KhyeOxR1ag]], shards [5]/[1] - reason [create index]
[22:14:20,369][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:20,809][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:20,809][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][1] creating shard
[22:14:20,810][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/R0SQI1RjTl-3KhyeOxR1ag/1, shard=[test][1]}]
[22:14:20,810][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [test][1]
[22:14:20,811][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:20,811][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:20,813][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:20,813][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][3] creating shard
[22:14:20,813][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#3]] starting recovery from store ...
[22:14:20,814][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/R0SQI1RjTl-3KhyeOxR1ag/3, shard=[test][3]}]
[22:14:20,814][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [test][3]
[22:14:20,814][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:20,815][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:20,815][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#3]] wipe translog location - creating new translog
[22:14:20,816][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:20,816][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][2] creating shard
[22:14:20,816][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#3]] no translog ID present in the current generation - creating one
[22:14:20,816][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/R0SQI1RjTl-3KhyeOxR1ag/2, shard=[test][2]}]
[22:14:20,816][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [test][2]
[22:14:20,817][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#1]] starting recovery from store ...
[22:14:20,817][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:20,817][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:20,818][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:20,818][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#1]] wipe translog location - creating new translog
[22:14:20,818][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][0] creating shard
[22:14:20,818][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#2]] starting recovery from store ...
[22:14:20,818][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/R0SQI1RjTl-3KhyeOxR1ag/0, shard=[test][0]}]
[22:14:20,819][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [test][0]
[22:14:20,819][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#1]] no translog ID present in the current generation - creating one
[22:14:20,819][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:20,819][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:20,820][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#2]] wipe translog location - creating new translog
[22:14:20,820][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:20,820][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:20,820][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#2]] no translog ID present in the current generation - creating one
[22:14:20,820][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#3]] recovery completed from [shard_store], took [10ms]
[22:14:20,820][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#4]] starting recovery from store ...
[22:14:20,821][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[test][1]], allocation id [YFa2TaBjQuWkM9ur0P1JKA], primary term [0], message [after new shard recovery]]
[22:14:20,821][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [YFa2TaBjQuWkM9ur0P1JKA], primary term [0], message [after new shard recovery]]
[22:14:20,822][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: took [923ms] done applying updated cluster_state (version: 3, uuid: R10p0o9_T6-M2DsmAzpPTg)
[22:14:20,822][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#4]] wipe translog location - creating new translog
[22:14:20,822][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [YFa2TaBjQuWkM9ur0P1JKA], primary term [0], message [after new shard recovery]]]: execute
[22:14:20,822][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=YFa2TaBjQuWkM9ur0P1JKA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:20.367Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [YFa2TaBjQuWkM9ur0P1JKA], primary term [0], message [after new shard recovery]])
[22:14:20,823][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#4]] no translog ID present in the current generation - creating one
[22:14:20,823][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:20,823][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:20,823][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#1]] recovery completed from [shard_store], took [9ms]
[22:14:20,823][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#1]] [test][3] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[test][3]], allocation id [QrBw09h-TWOmYBY7lQqYdw], primary term [0], message [after new shard recovery]]
[22:14:20,823][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [QrBw09h-TWOmYBY7lQqYdw], primary term [0], message [after new shard recovery]]
[22:14:20,823][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#2]] recovery completed from [shard_store], took [6ms]
[22:14:20,823][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#2]] [test][2] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[test][2]], allocation id [EdEIQ4EMQ1i1ELYx2XlRjw], primary term [0], message [after new shard recovery]]
[22:14:20,823][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#2]] [test][2] received shard started for [shard id [[test][2]], allocation id [EdEIQ4EMQ1i1ELYx2XlRjw], primary term [0], message [after new shard recovery]]
[22:14:20,823][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [YFa2TaBjQuWkM9ur0P1JKA], primary term [0], message [after new shard recovery]]]
[22:14:20,823][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [4]
[22:14:20,823][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 4
[22:14:20,825][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:20,825][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:20,825][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#4]] recovery completed from [shard_store], took [6ms]
[22:14:20,825][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[test][0]], allocation id [Lzj0Qh_uQAKZVFWAn9Gm2Q], primary term [0], message [after new shard recovery]]
[22:14:20,825][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[test][3]], allocation id [QrBw09h-TWOmYBY7lQqYdw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:20,825][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [Lzj0Qh_uQAKZVFWAn9Gm2Q], primary term [0], message [after new shard recovery]]
[22:14:20,825][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [QrBw09h-TWOmYBY7lQqYdw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:20,825][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][2] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[test][2]], allocation id [EdEIQ4EMQ1i1ELYx2XlRjw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:20,825][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [EdEIQ4EMQ1i1ELYx2XlRjw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:20,825][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][4] creating shard
[22:14:20,825][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/R0SQI1RjTl-3KhyeOxR1ag/4, shard=[test][4]}]
[22:14:20,825][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [test][4]
[22:14:20,826][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:20,826][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:20,827][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:20,827][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#3]] starting recovery from store ...
[22:14:20,827][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[test][0]], allocation id [Lzj0Qh_uQAKZVFWAn9Gm2Q], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:20,827][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [Lzj0Qh_uQAKZVFWAn9Gm2Q], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:20,829][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#3]] wipe translog location - creating new translog
[22:14:20,829][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [YFa2TaBjQuWkM9ur0P1JKA], primary term [0], message [after new shard recovery]]]: took [7ms] done applying updated cluster_state (version: 4, uuid: DPWo1O6qTl2_SlxVLfczcg)
[22:14:20,829][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [QrBw09h-TWOmYBY7lQqYdw], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [EdEIQ4EMQ1i1ELYx2XlRjw], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [Lzj0Qh_uQAKZVFWAn9Gm2Q], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [QrBw09h-TWOmYBY7lQqYdw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][2]], allocation id [EdEIQ4EMQ1i1ELYx2XlRjw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [Lzj0Qh_uQAKZVFWAn9Gm2Q], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:20,829][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=QrBw09h-TWOmYBY7lQqYdw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:20.367Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [QrBw09h-TWOmYBY7lQqYdw], primary term [0], message [after new shard recovery]])
[22:14:20,829][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=EdEIQ4EMQ1i1ELYx2XlRjw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:20.367Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [EdEIQ4EMQ1i1ELYx2XlRjw], primary term [0], message [after new shard recovery]])
[22:14:20,830][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=Lzj0Qh_uQAKZVFWAn9Gm2Q], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:20.367Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [Lzj0Qh_uQAKZVFWAn9Gm2Q], primary term [0], message [after new shard recovery]])
[22:14:20,830][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#3]] no translog ID present in the current generation - creating one
[22:14:20,831][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][3]], allocation id [QrBw09h-TWOmYBY7lQqYdw], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [EdEIQ4EMQ1i1ELYx2XlRjw], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [Lzj0Qh_uQAKZVFWAn9Gm2Q], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [QrBw09h-TWOmYBY7lQqYdw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][2]], allocation id [EdEIQ4EMQ1i1ELYx2XlRjw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [Lzj0Qh_uQAKZVFWAn9Gm2Q], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:20,831][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [5]
[22:14:20,831][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 5
[22:14:20,833][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:20,833][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:20,833][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:20,833][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#3]] recovery completed from [shard_store], took [7ms]
[22:14:20,833][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[test][4]], allocation id [iln4WjtDRImoZ5gQpKsKRw], primary term [0], message [after new shard recovery]]
[22:14:20,833][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[test][4]], allocation id [iln4WjtDRImoZ5gQpKsKRw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:20,833][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [iln4WjtDRImoZ5gQpKsKRw], primary term [0], message [after new shard recovery]]
[22:14:20,833][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [iln4WjtDRImoZ5gQpKsKRw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:20,833][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:20,835][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [QrBw09h-TWOmYBY7lQqYdw], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [EdEIQ4EMQ1i1ELYx2XlRjw], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [Lzj0Qh_uQAKZVFWAn9Gm2Q], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [QrBw09h-TWOmYBY7lQqYdw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][2]], allocation id [EdEIQ4EMQ1i1ELYx2XlRjw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [Lzj0Qh_uQAKZVFWAn9Gm2Q], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [6ms] done applying updated cluster_state (version: 5, uuid: ADjbE0W_TDuc-9h1WN_AeQ)
[22:14:20,836][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [iln4WjtDRImoZ5gQpKsKRw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][4]], allocation id [iln4WjtDRImoZ5gQpKsKRw], primary term [0], message [after new shard recovery]]]: execute
[22:14:20,836][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=iln4WjtDRImoZ5gQpKsKRw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:20.367Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [iln4WjtDRImoZ5gQpKsKRw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]])
[22:14:20,837][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [iln4WjtDRImoZ5gQpKsKRw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][4]], allocation id [iln4WjtDRImoZ5gQpKsKRw], primary term [0], message [after new shard recovery]]]
[22:14:20,837][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [6]
[22:14:20,838][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 6
[22:14:20,838][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:20,839][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [iln4WjtDRImoZ5gQpKsKRw], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][4]], allocation id [iln4WjtDRImoZ5gQpKsKRw], primary term [0], message [after new shard recovery]]]: took [3ms] done applying updated cluster_state (version: 6, uuid: 52nQ-QNqS3yshP2twLrlMw)
[22:14:20,841][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[22:14:21,318][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:21,321][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [test/R0SQI1RjTl-3KhyeOxR1ag] create_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:21,322][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [7], source [put-mapping[test]]
[22:14:21,322][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [7]
[22:14:21,322][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 7
[22:14:21,322][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [[test/R0SQI1RjTl-3KhyeOxR1ag]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:21,324][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [483ms] done applying updated cluster_state (version: 7, uuid: cCG9XcWrTSiN2uuhM29Vpw)
[22:14:21,342][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'authorities' index
[22:14:21,342][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: execute
[22:14:21,342][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating Index [[authorities/UMpv1ANERzySy0PPsqILJg]], shards [5]/[1] - reason [create index]
[22:14:21,342][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:21,796][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:21,797][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[22:14:21,798][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities] closing ... (reason [cleaning up after validating index on master])
[22:14:21,798][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities/UMpv1ANERzySy0PPsqILJg] closing index service (reason [cleaning up after validating index on master])
[22:14:21,798][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:21,798][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] full cache clear, reason [close]
[22:14:21,798][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:21,798][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities/UMpv1ANERzySy0PPsqILJg] closed... (reason [cleaning up after validating index on master])
[22:14:21,799][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [8], source [create-index [authorities], cause [auto(index api)]]
[22:14:21,799][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [8]
[22:14:21,799][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 8
[22:14:21,799][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [[authorities/UMpv1ANERzySy0PPsqILJg]] creating index
[22:14:21,799][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating Index [[authorities/UMpv1ANERzySy0PPsqILJg]], shards [5]/[1] - reason [create index]
[22:14:21,799][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:22,256][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:22,257][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][1] creating shard
[22:14:22,257][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/UMpv1ANERzySy0PPsqILJg/1, shard=[authorities][1]}]
[22:14:22,257][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [authorities][1]
[22:14:22,258][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:22,258][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:22,260][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:22,260][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][3] creating shard
[22:14:22,260][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#1]] starting recovery from store ...
[22:14:22,261][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/UMpv1ANERzySy0PPsqILJg/3, shard=[authorities][3]}]
[22:14:22,261][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [authorities][3]
[22:14:22,262][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:22,262][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:22,262][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#1]] wipe translog location - creating new translog
[22:14:22,263][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:22,263][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][2] creating shard
[22:14:22,263][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#2]] starting recovery from store ...
[22:14:22,263][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#1]] no translog ID present in the current generation - creating one
[22:14:22,263][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/UMpv1ANERzySy0PPsqILJg/2, shard=[authorities][2]}]
[22:14:22,263][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [authorities][2]
[22:14:22,264][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:22,264][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:22,264][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#2]] wipe translog location - creating new translog
[22:14:22,265][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:22,265][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][0] creating shard
[22:14:22,265][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#4]] starting recovery from store ...
[22:14:22,265][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#2]] no translog ID present in the current generation - creating one
[22:14:22,265][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/UMpv1ANERzySy0PPsqILJg/0, shard=[authorities][0]}]
[22:14:22,265][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [authorities][0]
[22:14:22,265][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:22,265][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#1]] recovery completed from [shard_store], took [8ms]
[22:14:22,265][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#1]] [authorities][1] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[authorities][1]], allocation id [XVsQkSbqR3O4-8RnCXujCg], primary term [0], message [after new shard recovery]]
[22:14:22,265][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#1]] [authorities][1] received shard started for [shard id [[authorities][1]], allocation id [XVsQkSbqR3O4-8RnCXujCg], primary term [0], message [after new shard recovery]]
[22:14:22,266][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:22,266][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:22,266][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#4]] wipe translog location - creating new translog
[22:14:22,266][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:22,266][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#3]] starting recovery from store ...
[22:14:22,266][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#4]] no translog ID present in the current generation - creating one
[22:14:22,266][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:22,266][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#2]] recovery completed from [shard_store], took [6ms]
[22:14:22,267][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#2]] [authorities][3] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[authorities][3]], allocation id [_d5ualLPTPODe4BrvhNbEw], primary term [0], message [after new shard recovery]]
[22:14:22,267][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#2]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [_d5ualLPTPODe4BrvhNbEw], primary term [0], message [after new shard recovery]]
[22:14:22,267][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#3]] wipe translog location - creating new translog
[22:14:22,268][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: took [925ms] done applying updated cluster_state (version: 8, uuid: W7ojoQCfTL-fwOxMKAYjhw)
[22:14:22,268][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [XVsQkSbqR3O4-8RnCXujCg], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [_d5ualLPTPODe4BrvhNbEw], primary term [0], message [after new shard recovery]]]: execute
[22:14:22,268][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][1] starting shard [authorities][1], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=XVsQkSbqR3O4-8RnCXujCg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:21.797Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][1]], allocation id [XVsQkSbqR3O4-8RnCXujCg], primary term [0], message [after new shard recovery]])
[22:14:22,268][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#3]] no translog ID present in the current generation - creating one
[22:14:22,268][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][3] starting shard [authorities][3], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=_d5ualLPTPODe4BrvhNbEw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:21.797Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][3]], allocation id [_d5ualLPTPODe4BrvhNbEw], primary term [0], message [after new shard recovery]])
[22:14:22,268][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:22,268][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#4]] recovery completed from [shard_store], took [5ms]
[22:14:22,268][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#4]] [authorities][2] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[authorities][2]], allocation id [owZ-3TAxSEuimjRem3kaNg], primary term [0], message [after new shard recovery]]
[22:14:22,268][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#4]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [owZ-3TAxSEuimjRem3kaNg], primary term [0], message [after new shard recovery]]
[22:14:22,269][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[authorities][1]], allocation id [XVsQkSbqR3O4-8RnCXujCg], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [_d5ualLPTPODe4BrvhNbEw], primary term [0], message [after new shard recovery]]]
[22:14:22,269][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [9]
[22:14:22,269][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 9
[22:14:22,270][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:22,270][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:22,270][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#3]] recovery completed from [shard_store], took [4ms]
[22:14:22,270][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#3]] [authorities][0] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[authorities][0]], allocation id [8fqhuTQORRSWKfgTXk0MHg], primary term [0], message [after new shard recovery]]
[22:14:22,270][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:22,270][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#3]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [8fqhuTQORRSWKfgTXk0MHg], primary term [0], message [after new shard recovery]]
[22:14:22,270][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][2] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[authorities][2]], allocation id [owZ-3TAxSEuimjRem3kaNg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:22,270][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [owZ-3TAxSEuimjRem3kaNg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:22,270][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][4] creating shard
[22:14:22,270][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/UMpv1ANERzySy0PPsqILJg/4, shard=[authorities][4]}]
[22:14:22,270][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [authorities][4]
[22:14:22,271][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:22,271][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:22,272][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:22,272][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][0] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[authorities][0]], allocation id [8fqhuTQORRSWKfgTXk0MHg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:22,272][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#1]] starting recovery from store ...
[22:14:22,272][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [8fqhuTQORRSWKfgTXk0MHg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:22,273][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#1]] wipe translog location - creating new translog
[22:14:22,273][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [XVsQkSbqR3O4-8RnCXujCg], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [_d5ualLPTPODe4BrvhNbEw], primary term [0], message [after new shard recovery]]]: took [5ms] done applying updated cluster_state (version: 9, uuid: -AA2uw3fTxmQleBFT5w8OA)
[22:14:22,273][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [owZ-3TAxSEuimjRem3kaNg], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [8fqhuTQORRSWKfgTXk0MHg], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [owZ-3TAxSEuimjRem3kaNg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [8fqhuTQORRSWKfgTXk0MHg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:22,273][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][2] starting shard [authorities][2], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=owZ-3TAxSEuimjRem3kaNg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:21.797Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][2]], allocation id [owZ-3TAxSEuimjRem3kaNg], primary term [0], message [after new shard recovery]])
[22:14:22,274][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][0] starting shard [authorities][0], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=8fqhuTQORRSWKfgTXk0MHg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:21.797Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][0]], allocation id [8fqhuTQORRSWKfgTXk0MHg], primary term [0], message [after new shard recovery]])
[22:14:22,274][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#1]] no translog ID present in the current generation - creating one
[22:14:22,275][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[authorities][2]], allocation id [owZ-3TAxSEuimjRem3kaNg], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [8fqhuTQORRSWKfgTXk0MHg], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [owZ-3TAxSEuimjRem3kaNg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [8fqhuTQORRSWKfgTXk0MHg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:22,275][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [10]
[22:14:22,275][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 10
[22:14:22,276][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:22,276][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#1]] recovery completed from [shard_store], took [5ms]
[22:14:22,276][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[authorities][4]], allocation id [HJFYd86GSf-1glxagFzBqA], primary term [0], message [after new shard recovery]]
[22:14:22,276][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:22,276][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [HJFYd86GSf-1glxagFzBqA], primary term [0], message [after new shard recovery]]
[22:14:22,276][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[authorities][4]], allocation id [HJFYd86GSf-1glxagFzBqA], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:22,276][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [HJFYd86GSf-1glxagFzBqA], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:22,276][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:22,278][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [owZ-3TAxSEuimjRem3kaNg], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [8fqhuTQORRSWKfgTXk0MHg], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [owZ-3TAxSEuimjRem3kaNg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [8fqhuTQORRSWKfgTXk0MHg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 10, uuid: sovEmghHQWypWnqiDMJ27A)
[22:14:22,278][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [HJFYd86GSf-1glxagFzBqA], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [HJFYd86GSf-1glxagFzBqA], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:22,278][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities][4] starting shard [authorities][4], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=HJFYd86GSf-1glxagFzBqA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:21.797Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[authorities][4]], allocation id [HJFYd86GSf-1glxagFzBqA], primary term [0], message [after new shard recovery]])
[22:14:22,280][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [11], source [shard-started[shard id [[authorities][4]], allocation id [HJFYd86GSf-1glxagFzBqA], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [HJFYd86GSf-1glxagFzBqA], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:22,280][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [11]
[22:14:22,280][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 11
[22:14:22,280][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:22,282][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [HJFYd86GSf-1glxagFzBqA], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [HJFYd86GSf-1glxagFzBqA], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [3ms] done applying updated cluster_state (version: 11, uuid: EXYSwX5sQweWjT4gtAIBlg)
[22:14:22,284][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: execute
[22:14:22,743][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:22,746][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [authorities/UMpv1ANERzySy0PPsqILJg] create_mapping [persons] with source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:22,746][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [12], source [put-mapping[persons]]
[22:14:22,746][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [12]
[22:14:22,746][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 12
[22:14:22,747][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [[authorities/UMpv1ANERzySy0PPsqILJg]] adding mapping [persons], source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:22,749][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: took [465ms] done applying updated cluster_state (version: 12, uuid: 2eJ0r_-1QJCQIhPbHIy96w)
[22:14:22,767][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete index 'books'
[22:14:22,768][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [create-index [books], cause [api]]: execute
[22:14:22,768][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating Index [[books/nidjTypCRQKfXg2Bf_vS7g]], shards [5]/[1] - reason [create index]
[22:14:22,768][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:23,225][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:23,227][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books] creating index, cause [api], templates [], shards [5]/[1], mappings [test]
[22:14:23,229][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books] closing ... (reason [cleaning up after validating index on master])
[22:14:23,229][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books/nidjTypCRQKfXg2Bf_vS7g] closing index service (reason [cleaning up after validating index on master])
[22:14:23,229][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:23,229][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] full cache clear, reason [close]
[22:14:23,230][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:14:23,230][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books/nidjTypCRQKfXg2Bf_vS7g] closed... (reason [cleaning up after validating index on master])
[22:14:23,230][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [13], source [create-index [books], cause [api]]
[22:14:23,231][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [13]
[22:14:23,231][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 13
[22:14:23,231][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [[books/nidjTypCRQKfXg2Bf_vS7g]] creating index
[22:14:23,232][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating Index [[books/nidjTypCRQKfXg2Bf_vS7g]], shards [5]/[1] - reason [create index]
[22:14:23,232][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:14:23,675][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:23,675][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [[books/nidjTypCRQKfXg2Bf_vS7g]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;authorID&quot;:{&quot;type&quot;:&quot;ref&quot;,&quot;type&quot;:&quot;ref&quot;,&quot;ref_index&quot;:&quot;authorities&quot;,&quot;ref_type&quot;:&quot;persons&quot;,&quot;ref_fields&quot;:[&quot;author&quot;],&quot;copy_to&quot;:[&quot;dc.creator&quot;,&quot;bib.contributor&quot;]},&quot;bib&quot;:{&quot;properties&quot;:{&quot;contributor&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;dc&quot;:{&quot;properties&quot;:{&quot;creator&quot;:{&quot;type&quot;:&quot;text&quot;}}}}}}]
[22:14:23,676][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][1] creating shard
[22:14:23,676][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/nidjTypCRQKfXg2Bf_vS7g/1, shard=[books][1]}]
[22:14:23,676][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [books][1]
[22:14:23,677][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:23,677][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:23,679][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:23,679][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][3] creating shard
[22:14:23,679][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#2]] starting recovery from store ...
[22:14:23,679][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/nidjTypCRQKfXg2Bf_vS7g/3, shard=[books][3]}]
[22:14:23,679][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [books][3]
[22:14:23,680][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:23,680][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:23,680][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#2]] wipe translog location - creating new translog
[22:14:23,681][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:23,681][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][2] creating shard
[22:14:23,681][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#4]] starting recovery from store ...
[22:14:23,681][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#2]] no translog ID present in the current generation - creating one
[22:14:23,681][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/nidjTypCRQKfXg2Bf_vS7g/2, shard=[books][2]}]
[22:14:23,681][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [books][2]
[22:14:23,682][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:23,682][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:23,682][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#4]] wipe translog location - creating new translog
[22:14:23,683][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:23,683][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][0] creating shard
[22:14:23,683][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#3]] starting recovery from store ...
[22:14:23,683][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/nidjTypCRQKfXg2Bf_vS7g/0, shard=[books][0]}]
[22:14:23,683][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [books][0]
[22:14:23,683][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#4]] no translog ID present in the current generation - creating one
[22:14:23,683][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:23,683][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:23,684][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:23,684][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#3]] wipe translog location - creating new translog
[22:14:23,684][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#2]] recovery completed from [shard_store], took [7ms]
[22:14:23,684][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#2]] [books][1] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[books][1]], allocation id [JM4rDtL5RweHbx9boZc9Zg], primary term [0], message [after new shard recovery]]
[22:14:23,684][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#2]] [books][1] received shard started for [shard id [[books][1]], allocation id [JM4rDtL5RweHbx9boZc9Zg], primary term [0], message [after new shard recovery]]
[22:14:23,684][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:23,684][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#1]] starting recovery from store ...
[22:14:23,684][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#3]] no translog ID present in the current generation - creating one
[22:14:23,685][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#1]] wipe translog location - creating new translog
[22:14:23,685][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:23,685][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#4]] recovery completed from [shard_store], took [6ms]
[22:14:23,685][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#4]] [books][3] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[books][3]], allocation id [6W_kfdyMQEi6txDzEoeQWw], primary term [0], message [after new shard recovery]]
[22:14:23,685][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#4]] [books][3] received shard started for [shard id [[books][3]], allocation id [6W_kfdyMQEi6txDzEoeQWw], primary term [0], message [after new shard recovery]]
[22:14:23,685][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [create-index [books], cause [api]]: took [917ms] done applying updated cluster_state (version: 13, uuid: RGXXjsLbQv6wppjo-lMcpA)
[22:14:23,685][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][1]], allocation id [JM4rDtL5RweHbx9boZc9Zg], primary term [0], message [after new shard recovery], shard id [[books][3]], allocation id [6W_kfdyMQEi6txDzEoeQWw], primary term [0], message [after new shard recovery]]]: execute
[22:14:23,685][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][1] starting shard [books][1], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=JM4rDtL5RweHbx9boZc9Zg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:23.227Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[books][1]], allocation id [JM4rDtL5RweHbx9boZc9Zg], primary term [0], message [after new shard recovery]])
[22:14:23,686][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#1]] no translog ID present in the current generation - creating one
[22:14:23,686][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][3] starting shard [books][3], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=6W_kfdyMQEi6txDzEoeQWw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:23.227Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[books][3]], allocation id [6W_kfdyMQEi6txDzEoeQWw], primary term [0], message [after new shard recovery]])
[22:14:23,686][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:23,686][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#3]] recovery completed from [shard_store], took [5ms]
[22:14:23,686][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#3]] [books][2] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[books][2]], allocation id [9m83EYVnTImAcf3CO0V3rg], primary term [0], message [after new shard recovery]]
[22:14:23,686][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#3]] [books][2] received shard started for [shard id [[books][2]], allocation id [9m83EYVnTImAcf3CO0V3rg], primary term [0], message [after new shard recovery]]
[22:14:23,687][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [14], source [shard-started[shard id [[books][1]], allocation id [JM4rDtL5RweHbx9boZc9Zg], primary term [0], message [after new shard recovery], shard id [[books][3]], allocation id [6W_kfdyMQEi6txDzEoeQWw], primary term [0], message [after new shard recovery]]]
[22:14:23,687][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [14]
[22:14:23,687][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 14
[22:14:23,688][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:23,688][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#1]] recovery completed from [shard_store], took [4ms]
[22:14:23,688][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#1]] [books][0] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[books][0]], allocation id [nELeC_noRgOT9mbFUlYlGg], primary term [0], message [after new shard recovery]]
[22:14:23,688][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:23,688][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#1]] [books][0] received shard started for [shard id [[books][0]], allocation id [nELeC_noRgOT9mbFUlYlGg], primary term [0], message [after new shard recovery]]
[22:14:23,688][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:23,688][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][2] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[books][2]], allocation id [9m83EYVnTImAcf3CO0V3rg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:23,688][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][2] received shard started for [shard id [[books][2]], allocation id [9m83EYVnTImAcf3CO0V3rg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:23,688][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][4] creating shard
[22:14:23,689][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/nidjTypCRQKfXg2Bf_vS7g/4, shard=[books][4]}]
[22:14:23,689][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] creating shard_id [books][4]
[22:14:23,689][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:14:23,690][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]
[22:14:23,691][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:14:23,691][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][0] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[books][0]], allocation id [nELeC_noRgOT9mbFUlYlGg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:23,691][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][0] received shard started for [shard id [[books][0]], allocation id [nELeC_noRgOT9mbFUlYlGg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:23,691][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#2]] starting recovery from store ...
[22:14:23,692][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][1]], allocation id [JM4rDtL5RweHbx9boZc9Zg], primary term [0], message [after new shard recovery], shard id [[books][3]], allocation id [6W_kfdyMQEi6txDzEoeQWw], primary term [0], message [after new shard recovery]]]: took [6ms] done applying updated cluster_state (version: 14, uuid: 7hmGk676TceIRhJ8Pt1HQg)
[22:14:23,692][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][2]], allocation id [9m83EYVnTImAcf3CO0V3rg], primary term [0], message [after new shard recovery], shard id [[books][0]], allocation id [nELeC_noRgOT9mbFUlYlGg], primary term [0], message [after new shard recovery], shard id [[books][2]], allocation id [9m83EYVnTImAcf3CO0V3rg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[books][0]], allocation id [nELeC_noRgOT9mbFUlYlGg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:23,692][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][2] starting shard [books][2], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=9m83EYVnTImAcf3CO0V3rg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:23.227Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[books][2]], allocation id [9m83EYVnTImAcf3CO0V3rg], primary term [0], message [after new shard recovery]])
[22:14:23,692][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[nokQigQ][generic][T#2]] wipe translog location - creating new translog
[22:14:23,692][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][0] starting shard [books][0], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=nELeC_noRgOT9mbFUlYlGg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:23.227Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[books][0]], allocation id [nELeC_noRgOT9mbFUlYlGg], primary term [0], message [after new shard recovery]])
[22:14:23,694][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[nokQigQ][generic][T#2]] no translog ID present in the current generation - creating one
[22:14:23,695][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [15], source [shard-started[shard id [[books][2]], allocation id [9m83EYVnTImAcf3CO0V3rg], primary term [0], message [after new shard recovery], shard id [[books][0]], allocation id [nELeC_noRgOT9mbFUlYlGg], primary term [0], message [after new shard recovery], shard id [[books][2]], allocation id [9m83EYVnTImAcf3CO0V3rg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[books][0]], allocation id [nELeC_noRgOT9mbFUlYlGg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:23,695][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [15]
[22:14:23,695][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 15
[22:14:23,699][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:14:23,699][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:23,699][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][generic][T#2]] recovery completed from [shard_store], took [10ms]
[22:14:23,699][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#2]] [books][4] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[books][4]], allocation id [BJ_D9jSWT6Kr0onyclCejQ], primary term [0], message [after new shard recovery]]
[22:14:23,699][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][generic][T#2]] [books][4] received shard started for [shard id [[books][4]], allocation id [BJ_D9jSWT6Kr0onyclCejQ], primary term [0], message [after new shard recovery]]
[22:14:23,699][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][4] sending [internal:cluster/shard/started] to [nokQigQlTymAHf0u0raGbQ] for shard entry [shard id [[books][4]], allocation id [BJ_D9jSWT6Kr0onyclCejQ], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:23,699][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][4] received shard started for [shard id [[books][4]], allocation id [BJ_D9jSWT6Kr0onyclCejQ], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:14:23,699][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:23,701][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][2]], allocation id [9m83EYVnTImAcf3CO0V3rg], primary term [0], message [after new shard recovery], shard id [[books][0]], allocation id [nELeC_noRgOT9mbFUlYlGg], primary term [0], message [after new shard recovery], shard id [[books][2]], allocation id [9m83EYVnTImAcf3CO0V3rg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[books][0]], allocation id [nELeC_noRgOT9mbFUlYlGg], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [8ms] done applying updated cluster_state (version: 15, uuid: DvHxOKMZQEyrN03EptsPVw)
[22:14:23,701][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][4]], allocation id [BJ_D9jSWT6Kr0onyclCejQ], primary term [0], message [after new shard recovery], shard id [[books][4]], allocation id [BJ_D9jSWT6Kr0onyclCejQ], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:14:23,701][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books][4] starting shard [books][4], node[nokQigQlTymAHf0u0raGbQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=BJ_D9jSWT6Kr0onyclCejQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:14:23.227Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[books][4]], allocation id [BJ_D9jSWT6Kr0onyclCejQ], primary term [0], message [after new shard recovery]])
[22:14:23,702][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [16], source [shard-started[shard id [[books][4]], allocation id [BJ_D9jSWT6Kr0onyclCejQ], primary term [0], message [after new shard recovery], shard id [[books][4]], allocation id [BJ_D9jSWT6Kr0onyclCejQ], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:14:23,702][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [16]
[22:14:23,702][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 16
[22:14:23,702][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:14:23,704][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][4]], allocation id [BJ_D9jSWT6Kr0onyclCejQ], primary term [0], message [after new shard recovery], shard id [[books][4]], allocation id [BJ_D9jSWT6Kr0onyclCejQ], primary term [0], message [master {nokQigQ}{nokQigQlTymAHf0u0raGbQ}{njyf2lyQQWi0iKOytDUFjA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 16, uuid: e9mwXwB8Q5ufPZ_t430P-g)
[22:14:23,706][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[22:14:24,204][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] using dynamic[true]
[22:14:24,206][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [books/nidjTypCRQKfXg2Bf_vS7g] update_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;authorID&quot;:{&quot;type&quot;:&quot;ref&quot;,&quot;type&quot;:&quot;ref&quot;,&quot;ref_index&quot;:&quot;authorities&quot;,&quot;ref_type&quot;:&quot;persons&quot;,&quot;ref_fields&quot;:[&quot;author&quot;],&quot;copy_to&quot;:[&quot;dc.creator&quot;,&quot;bib.contributor&quot;]},&quot;bib&quot;:{&quot;properties&quot;:{&quot;contributor&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;dc&quot;:{&quot;properties&quot;:{&quot;creator&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;title&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:24,207][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] cluster state updated, version [17], source [put-mapping[test]]
[22:14:24,207][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] publishing cluster state version [17]
[22:14:24,207][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] set local cluster state to version 17
[22:14:24,207][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] [[books/nidjTypCRQKfXg2Bf_vS7g]] updating mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;authorID&quot;:{&quot;type&quot;:&quot;ref&quot;,&quot;type&quot;:&quot;ref&quot;,&quot;ref_index&quot;:&quot;authorities&quot;,&quot;ref_type&quot;:&quot;persons&quot;,&quot;ref_fields&quot;:[&quot;author&quot;],&quot;copy_to&quot;:[&quot;dc.creator&quot;,&quot;bib.contributor&quot;]},&quot;bib&quot;:{&quot;properties&quot;:{&quot;contributor&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;dc&quot;:{&quot;properties&quot;:{&quot;creator&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;title&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[22:14:24,210][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[nokQigQ][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [503ms] done applying updated cluster_state (version: 17, uuid: x9Gj2KuLQreC3Noi4M5WXQ)
[22:14:24,234][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] mappings={properties={authorID={type=ref, ref_index=authorities, ref_type=persons, ref_fields=[author], copy_to=[dc.creator, bib.contributor]}, bib={properties={contributor={type=text}}}, dc={properties={creator={type=text}}}, title={type=text, fields={keyword={type=keyword, ignore_above=256}}}}}
[22:14:24,236][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unref hits = 1
[22:14:24,236][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] {bib={contributor=A contributor}, title=A title, authorID=1, dc={creator=A creator}}
[22:14:24,238][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] ref hits = 1
[22:14:24,238][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] {bib={contributor=A contributor}, title=A title, authorID=1, dc={creator=A creator}}
[22:14:24,240][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] field 2 unref hits = 1
[22:14:24,240][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] {bib={contributor=A contributor}, title=A title, authorID=1, dc={creator=A creator}}
[22:14:24,242][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] field 2 ref hits = 1
[22:14:24,243][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] {bib={contributor=A contributor}, title=A title, authorID=1, dc={creator=A creator}}
[22:14:24,243][INFO ][test                     ][Test worker] stopping nodes
[22:14:24,243][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[22:14:24,243][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [authorities] closing ... (reason [shutdown])
[22:14:24,244][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [authorities/UMpv1ANERzySy0PPsqILJg] closing index service (reason [shutdown])
[22:14:24,244][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[22:14:24,244][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#3]] [books] closing ... (reason [shutdown])
[22:14:24,244][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,244][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,244][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#3]] [books/nidjTypCRQKfXg2Bf_vS7g] closing index service (reason [shutdown])
[22:14:24,244][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [0] closing... (reason: [shutdown])
[22:14:24,244][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:24,244][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:24,244][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test] closing ... (reason [shutdown])
[22:14:24,244][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,244][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:24,244][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,244][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test/R0SQI1RjTl-3KhyeOxR1ag] closing index service (reason [shutdown])
[22:14:24,244][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[22:14:24,244][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[22:14:24,244][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[22:14:24,244][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[22:14:24,244][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,244][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:24,244][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,245][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:24,245][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:24,245][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:24,245][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[22:14:24,245][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[22:14:24,245][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:24,245][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[22:14:24,245][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,245][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,245][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:24,245][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:24,245][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[22:14:24,245][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [0] closed (reason: [shutdown])
[22:14:24,245][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:24,245][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [1] closing... (reason: [shutdown])
[22:14:24,245][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:24,245][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,245][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:24,245][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:24,245][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,245][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:24,245][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[22:14:24,245][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[22:14:24,245][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[22:14:24,245][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[22:14:24,245][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[22:14:24,245][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,246][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,245][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:24,246][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:24,246][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:24,246][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[22:14:24,246][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[22:14:24,246][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [1] closed (reason: [shutdown])
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:24,246][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [2] closing... (reason: [shutdown])
[22:14:24,246][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:24,246][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,246][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,246][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[22:14:24,246][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[22:14:24,246][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,246][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,246][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[22:14:24,246][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:24,246][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:24,246][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,247][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:24,247][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[22:14:24,247][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[22:14:24,247][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:24,247][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [2] closed (reason: [shutdown])
[22:14:24,247][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [3] closing... (reason: [shutdown])
[22:14:24,247][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:24,247][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[22:14:24,247][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[22:14:24,247][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,247][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,247][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,247][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,247][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:24,247][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:24,247][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:24,247][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:24,247][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:24,247][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[22:14:24,247][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[22:14:24,247][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,247][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,252][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[22:14:24,252][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:24,252][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[22:14:24,252][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:24,252][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[22:14:24,252][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:24,253][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[22:14:24,253][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:24,253][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[22:14:24,253][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:24,253][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[22:14:24,253][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[22:14:24,253][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[22:14:24,253][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[22:14:24,253][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,253][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[22:14:24,253][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,253][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[22:14:24,253][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:14:24,253][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:14:24,253][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:14:24,253][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test/R0SQI1RjTl-3KhyeOxR1ag] closed... (reason [shutdown])
[22:14:24,254][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[22:14:24,254][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:14:24,254][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[22:14:24,254][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:14:24,254][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[22:14:24,254][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[22:14:24,254][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:14:24,254][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[22:14:24,254][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:14:24,254][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[22:14:24,254][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[22:14:24,254][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [authorities/UMpv1ANERzySy0PPsqILJg] closed... (reason [shutdown])
[22:14:24,254][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [3] closed (reason: [shutdown])
[22:14:24,254][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [4] closing... (reason: [shutdown])
[22:14:24,254][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:14:24,254][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[22:14:24,254][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[22:14:24,254][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[22:14:24,254][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[22:14:24,254][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[22:14:24,254][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[22:14:24,254][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [4] closed (reason: [shutdown])
[22:14:24,254][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#3]] clearing all bitsets because [close]
[22:14:24,255][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#3]] full cache clear, reason [close]
[22:14:24,255][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#3]] clearing all bitsets because [close]
[22:14:24,255][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#3]] [books/nidjTypCRQKfXg2Bf_vS7g] closed... (reason [shutdown])
[22:14:24,255][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[22:14:24,255][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[22:14:24,256][INFO ][org.elasticsearch.node.Node][Test worker] closed
[22:14:24,273][INFO ][test                     ][Test worker] data files wiped
</pre>
</span>
</div>
</div>
<div id="footer">
<p>
<div>
<label class="hidden" id="label-for-line-wrapping-toggle" for="line-wrapping-toggle">Wrap lines
<input id="line-wrapping-toggle" type="checkbox" autocomplete="off"/>
</label>
</div>Generated by 
<a href="http://www.gradle.org">Gradle 3.2.1</a> at 25.01.2017 22:14:33</p>
</div>
</div>
</body>
</html>
