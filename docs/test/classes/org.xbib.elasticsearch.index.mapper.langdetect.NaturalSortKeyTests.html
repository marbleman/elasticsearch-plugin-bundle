<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=edge"/>
<title>Test results - Class org.xbib.elasticsearch.index.mapper.langdetect.NaturalSortKeyTests</title>
<link href="../css/base-style.css" rel="stylesheet" type="text/css"/>
<link href="../css/style.css" rel="stylesheet" type="text/css"/>
<script src="../js/report.js" type="text/javascript"></script>
</head>
<body>
<div id="content">
<h1>Class org.xbib.elasticsearch.index.mapper.langdetect.NaturalSortKeyTests</h1>
<div class="breadcrumbs">
<a href="../index.html">all</a> &gt; 
<a href="../packages/org.xbib.elasticsearch.index.mapper.langdetect.html">org.xbib.elasticsearch.index.mapper.langdetect</a> &gt; NaturalSortKeyTests</div>
<div id="summary">
<table>
<tr>
<td>
<div class="summaryGroup">
<table>
<tr>
<td>
<div class="infoBox" id="tests">
<div class="counter">3</div>
<p>tests</p>
</div>
</td>
<td>
<div class="infoBox" id="failures">
<div class="counter">0</div>
<p>failures</p>
</div>
</td>
<td>
<div class="infoBox" id="ignored">
<div class="counter">0</div>
<p>ignored</p>
</div>
</td>
<td>
<div class="infoBox" id="duration">
<div class="counter">1m49.03s</div>
<p>duration</p>
</div>
</td>
</tr>
</table>
</div>
</td>
<td>
<div class="infoBox success" id="successRate">
<div class="percent">100%</div>
<p>successful</p>
</div>
</td>
</tr>
</table>
</div>
<div id="tabs">
<ul class="tabLinks">
<li>
<a href="#tab0">Tests</a>
</li>
<li>
<a href="#tab1">Standard output</a>
</li>
</ul>
<div id="tab0" class="tab">
<h2>Tests</h2>
<table>
<thead>
<tr>
<th>Test</th>
<th>Duration</th>
<th>Result</th>
</tr>
</thead>
<tr>
<td class="success">testComplex</td>
<td>36.279s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testDewey</td>
<td>36.472s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testSort</td>
<td>36.280s</td>
<td class="success">passed</td>
</tr>
</table>
</div>
<div id="tab1" class="tab">
<h2>Standard output</h2>
<span class="code">
<pre>[22:11:54,027][INFO ][test                     ][Test worker] settings cluster name
[22:11:54,027][INFO ][test                     ][Test worker] starting nodes
[22:11:54,027][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[22:11:54,028][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[22:11:54,133][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[22:11:54,133][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [138.5gb], usable_space [138.3gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[22:11:54,133][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[22:11:54,133][INFO ][org.elasticsearch.node.Node][Test worker] node name [8-CTczJ] derived from node ID [8-CTczJAQz-SCMP9Fmn2Yg]; set [node.name] to override
[22:11:54,133][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[5199], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[22:11:54,133][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[22:11:54,134][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[22:11:54,134][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[22:11:54,134][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[22:11:54,134][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[22:11:54,134][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[22:11:54,134][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[22:11:54,135][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[22:11:54,135][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[22:11:54,135][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[22:11:54,135][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[22:11:54,135][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[22:11:54,135][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[22:11:54,135][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[22:11:54,135][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[22:11:54,135][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[22:11:54,135][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[22:11:54,135][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[22:11:54,136][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[22:11:54,139][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:45e7:7aff:1af8:a30a prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[22:11:54,139][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[22:11:54,139][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[22:11:54,139][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[22:11:54,140][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[22:11:54,140][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[22:11:54,140][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[22:11:54,140][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[22:11:54,140][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[22:11:54,141][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[22:11:54,141][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[22:11:54,141][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[22:11:54,142][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[22:11:54,142][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[22:11:54,142][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[22:11:54,142][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[22:11:54,142][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:11:54,142][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:11:54,169][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[22:11:54,175][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[22:11:54,357][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[22:11:54,361][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[22:11:54,362][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[22:11:54,362][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[22:11:54,362][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[7]}, bound_addresses {local[7]}
[22:11:54,363][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[22:11:54,363][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [initial_join]: execute
[22:11:54,363][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[22:11:57,367][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[8-CTczJ][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]}], id[49], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[22:11:57,368][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[8-CTczJ][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[22:11:57,368][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[22:11:57,369][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[22:11:57,369][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] new_master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[22:11:57,369][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] publishing cluster state version [1]
[22:11:57,369][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] set local cluster state to version 1
[22:11:57,369][INFO ][org.elasticsearch.node.Node][Test worker] started
[22:11:57,370][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [1ms] done applying updated cluster_state (version: 1, uuid: K79AvqrbQO-RnzpdcejA5A)
[22:11:57,370][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[22:11:57,371][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[22:11:57,371][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] publishing cluster state version [2]
[22:11:57,371][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] set local cluster state to version 2
[22:11:57,373][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[22:11:57,373][INFO ][test                     ][Test worker] nodes are started
[22:11:57,373][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [2ms] done applying updated cluster_state (version: 2, uuid: sSdjG8YNQYOcxMR5pmTZSQ)
[22:11:57,373][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [create-index [test], cause [api]]: execute
[22:11:57,374][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] creating Index [[test/GcBWwH6aRd-tTfW8CQOKcQ]], shards [5]/[1] - reason [create index]
[22:11:57,374][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:11:57,854][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] using dynamic[true]
[22:11:57,858][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test] creating index, cause [api], templates [], shards [5]/[1], mappings [type1]
[22:11:57,859][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[22:11:57,859][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test/GcBWwH6aRd-tTfW8CQOKcQ] closing index service (reason [cleaning up after validating index on master])
[22:11:57,860][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:11:57,860][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] full cache clear, reason [close]
[22:11:57,860][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:11:57,860][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test/GcBWwH6aRd-tTfW8CQOKcQ] closed... (reason [cleaning up after validating index on master])
[22:11:57,860][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [api]]
[22:11:57,860][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] publishing cluster state version [3]
[22:11:57,860][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] set local cluster state to version 3
[22:11:57,860][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [[test/GcBWwH6aRd-tTfW8CQOKcQ]] creating index
[22:11:57,861][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] creating Index [[test/GcBWwH6aRd-tTfW8CQOKcQ]], shards [5]/[1] - reason [create index]
[22:11:57,861][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:11:58,314][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] using dynamic[true]
[22:11:58,315][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [[test/GcBWwH6aRd-tTfW8CQOKcQ]] adding mapping [type1], source [{&quot;type1&quot;:{&quot;properties&quot;:{&quot;notation&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;store&quot;:true,&quot;fields&quot;:{&quot;sort&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;:&quot;naturalsort&quot;,&quot;fielddata&quot;:true}}}}}}]
[22:11:58,316][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][1] creating shard
[22:11:58,317][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/GcBWwH6aRd-tTfW8CQOKcQ/1, shard=[test][1]}]
[22:11:58,317][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] creating shard_id [test][1]
[22:11:58,317][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:11:58,318][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [CREATED]
[22:11:58,318][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:11:58,318][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][3] creating shard
[22:11:58,318][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#3]] starting recovery from store ...
[22:11:58,319][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/GcBWwH6aRd-tTfW8CQOKcQ/3, shard=[test][3]}]
[22:11:58,319][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] creating shard_id [test][3]
[22:11:58,319][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:11:58,319][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [CREATED]
[22:11:58,320][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[8-CTczJ][generic][T#3]] wipe translog location - creating new translog
[22:11:58,320][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:11:58,320][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][2] creating shard
[22:11:58,320][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#1]] starting recovery from store ...
[22:11:58,321][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/GcBWwH6aRd-tTfW8CQOKcQ/2, shard=[test][2]}]
[22:11:58,321][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] creating shard_id [test][2]
[22:11:58,321][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[8-CTczJ][generic][T#3]] no translog ID present in the current generation - creating one
[22:11:58,321][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:11:58,322][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [CREATED]
[22:11:58,322][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[8-CTczJ][generic][T#1]] wipe translog location - creating new translog
[22:11:58,323][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:11:58,323][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][0] creating shard
[22:11:58,323][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[8-CTczJ][generic][T#1]] no translog ID present in the current generation - creating one
[22:11:58,323][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/GcBWwH6aRd-tTfW8CQOKcQ/0, shard=[test][0]}]
[22:11:58,323][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#2]] starting recovery from store ...
[22:11:58,323][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] creating shard_id [test][0]
[22:11:58,324][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:11:58,325][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [CREATED]
[22:11:58,326][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[8-CTczJ][generic][T#2]] wipe translog location - creating new translog
[22:11:58,327][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:11:58,327][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#4]] starting recovery from store ...
[22:11:58,327][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[8-CTczJ][generic][T#2]] no translog ID present in the current generation - creating one
[22:11:58,328][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [create-index [test], cause [api]]: took [954ms] done applying updated cluster_state (version: 3, uuid: imay6qoESni24umG6PffAw)
[22:11:58,329][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[8-CTczJ][generic][T#4]] wipe translog location - creating new translog
[22:11:58,329][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:11:58,329][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#3]] recovery completed from [shard_store], took [12ms]
[22:11:58,329][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [8-CTczJAQz-SCMP9Fmn2Yg] for shard entry [shard id [[test][1]], allocation id [v9alC7jvROmPfQqz0DBhJA], primary term [0], message [after new shard recovery]]
[22:11:58,329][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [v9alC7jvROmPfQqz0DBhJA], primary term [0], message [after new shard recovery]]
[22:11:58,330][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:11:58,330][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#1]] recovery completed from [shard_store], took [11ms]
[22:11:58,330][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][generic][T#1]] [test][3] sending [internal:cluster/shard/started] to [8-CTczJAQz-SCMP9Fmn2Yg] for shard entry [shard id [[test][3]], allocation id [kfHBgUpgQXm4p8PAyO1yVw], primary term [0], message [after new shard recovery]]
[22:11:58,330][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[8-CTczJ][generic][T#4]] no translog ID present in the current generation - creating one
[22:11:58,330][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][generic][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [kfHBgUpgQXm4p8PAyO1yVw], primary term [0], message [after new shard recovery]]
[22:11:58,330][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [v9alC7jvROmPfQqz0DBhJA], primary term [0], message [after new shard recovery]]]: execute
[22:11:58,330][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[8-CTczJAQz-SCMP9Fmn2Yg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=v9alC7jvROmPfQqz0DBhJA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:11:57.858Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [v9alC7jvROmPfQqz0DBhJA], primary term [0], message [after new shard recovery]])
[22:11:58,331][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:11:58,331][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#2]] recovery completed from [shard_store], took [10ms]
[22:11:58,331][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][generic][T#2]] [test][2] sending [internal:cluster/shard/started] to [8-CTczJAQz-SCMP9Fmn2Yg] for shard entry [shard id [[test][2]], allocation id [SkaICHMATK2h2G5ig4Zrjw], primary term [0], message [after new shard recovery]]
[22:11:58,331][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][generic][T#2]] [test][2] received shard started for [shard id [[test][2]], allocation id [SkaICHMATK2h2G5ig4Zrjw], primary term [0], message [after new shard recovery]]
[22:11:58,332][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [v9alC7jvROmPfQqz0DBhJA], primary term [0], message [after new shard recovery]]]
[22:11:58,332][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] publishing cluster state version [4]
[22:11:58,332][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] set local cluster state to version 4
[22:11:58,332][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:11:58,333][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:11:58,333][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#4]] recovery completed from [shard_store], took [9ms]
[22:11:58,333][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [8-CTczJAQz-SCMP9Fmn2Yg] for shard entry [shard id [[test][0]], allocation id [NPfRrA41T96zRC2NMMAkXw], primary term [0], message [after new shard recovery]]
[22:11:58,333][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [8-CTczJAQz-SCMP9Fmn2Yg] for shard entry [shard id [[test][3]], allocation id [kfHBgUpgQXm4p8PAyO1yVw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:11:58,333][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [NPfRrA41T96zRC2NMMAkXw], primary term [0], message [after new shard recovery]]
[22:11:58,333][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [kfHBgUpgQXm4p8PAyO1yVw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:11:58,333][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][2] sending [internal:cluster/shard/started] to [8-CTczJAQz-SCMP9Fmn2Yg] for shard entry [shard id [[test][2]], allocation id [SkaICHMATK2h2G5ig4Zrjw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:11:58,333][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [SkaICHMATK2h2G5ig4Zrjw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:11:58,333][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][4] creating shard
[22:11:58,333][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/GcBWwH6aRd-tTfW8CQOKcQ/4, shard=[test][4]}]
[22:11:58,334][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] creating shard_id [test][4]
[22:11:58,334][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:11:58,334][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [CREATED]
[22:11:58,335][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:11:58,335][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#3]] starting recovery from store ...
[22:11:58,335][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [8-CTczJAQz-SCMP9Fmn2Yg] for shard entry [shard id [[test][0]], allocation id [NPfRrA41T96zRC2NMMAkXw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:11:58,335][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [NPfRrA41T96zRC2NMMAkXw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:11:58,336][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[8-CTczJ][generic][T#3]] wipe translog location - creating new translog
[22:11:58,336][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [v9alC7jvROmPfQqz0DBhJA], primary term [0], message [after new shard recovery]]]: took [6ms] done applying updated cluster_state (version: 4, uuid: PHhQs376SJydB6TxLLcOLA)
[22:11:58,337][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [kfHBgUpgQXm4p8PAyO1yVw], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [SkaICHMATK2h2G5ig4Zrjw], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [NPfRrA41T96zRC2NMMAkXw], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [kfHBgUpgQXm4p8PAyO1yVw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][2]], allocation id [SkaICHMATK2h2G5ig4Zrjw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [NPfRrA41T96zRC2NMMAkXw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:11:58,337][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[8-CTczJAQz-SCMP9Fmn2Yg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=kfHBgUpgQXm4p8PAyO1yVw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:11:57.858Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [kfHBgUpgQXm4p8PAyO1yVw], primary term [0], message [after new shard recovery]])
[22:11:58,337][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[8-CTczJAQz-SCMP9Fmn2Yg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=SkaICHMATK2h2G5ig4Zrjw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:11:57.858Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [SkaICHMATK2h2G5ig4Zrjw], primary term [0], message [after new shard recovery]])
[22:11:58,337][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[8-CTczJ][generic][T#3]] no translog ID present in the current generation - creating one
[22:11:58,337][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[8-CTczJAQz-SCMP9Fmn2Yg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=NPfRrA41T96zRC2NMMAkXw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:11:57.858Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [NPfRrA41T96zRC2NMMAkXw], primary term [0], message [after new shard recovery]])
[22:11:58,339][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][3]], allocation id [kfHBgUpgQXm4p8PAyO1yVw], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [SkaICHMATK2h2G5ig4Zrjw], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [NPfRrA41T96zRC2NMMAkXw], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [kfHBgUpgQXm4p8PAyO1yVw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][2]], allocation id [SkaICHMATK2h2G5ig4Zrjw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [NPfRrA41T96zRC2NMMAkXw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:11:58,339][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] publishing cluster state version [5]
[22:11:58,339][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] set local cluster state to version 5
[22:11:58,340][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:11:58,340][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:11:58,340][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][generic][T#3]] recovery completed from [shard_store], took [7ms]
[22:11:58,341][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [8-CTczJAQz-SCMP9Fmn2Yg] for shard entry [shard id [[test][4]], allocation id [HxYDyW_oRamSTU9w334-nw], primary term [0], message [after new shard recovery]]
[22:11:58,341][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:11:58,341][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [HxYDyW_oRamSTU9w334-nw], primary term [0], message [after new shard recovery]]
[22:11:58,341][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [8-CTczJAQz-SCMP9Fmn2Yg] for shard entry [shard id [[test][4]], allocation id [HxYDyW_oRamSTU9w334-nw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:11:58,341][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [HxYDyW_oRamSTU9w334-nw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:11:58,341][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:11:58,342][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [kfHBgUpgQXm4p8PAyO1yVw], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [SkaICHMATK2h2G5ig4Zrjw], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [NPfRrA41T96zRC2NMMAkXw], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [kfHBgUpgQXm4p8PAyO1yVw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][2]], allocation id [SkaICHMATK2h2G5ig4Zrjw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [NPfRrA41T96zRC2NMMAkXw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [5ms] done applying updated cluster_state (version: 5, uuid: NJvEulRFQ1id7rb0VAGBMA)
[22:11:58,342][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [HxYDyW_oRamSTU9w334-nw], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [HxYDyW_oRamSTU9w334-nw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:11:58,343][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[8-CTczJAQz-SCMP9Fmn2Yg], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=HxYDyW_oRamSTU9w334-nw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:11:57.858Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [HxYDyW_oRamSTU9w334-nw], primary term [0], message [after new shard recovery]])
[22:11:58,343][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [HxYDyW_oRamSTU9w334-nw], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [HxYDyW_oRamSTU9w334-nw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:11:58,343][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] publishing cluster state version [6]
[22:11:58,344][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] set local cluster state to version 6
[22:11:58,344][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:11:58,345][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[8-CTczJ][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [HxYDyW_oRamSTU9w334-nw], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [HxYDyW_oRamSTU9w334-nw], primary term [0], message [master {8-CTczJ}{8-CTczJAQz-SCMP9Fmn2Yg}{hS5-j-QVTNubyI0OYhab8w}{local}{local[7]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 6, uuid: _D5fLofrQ6yYqqkXpr8-VQ)
[22:12:27,373][INFO ][org.elasticsearch.cluster.routing.allocation.DiskThresholdMonitor][elasticsearch[8-CTczJ][management][T#2]] low disk watermark [85%] exceeded on [8-CTczJAQz-SCMP9Fmn2Yg][8-CTczJ][/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0] free: 138.3gb[14.8%], replicas will not be assigned to this node
[22:12:28,430][DEBUG][org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData][elasticsearch[8-CTczJ][warmer][T#1]] global-ordinals [_parent] took [1.6ms]
[22:12:28,466][INFO ][test                     ][Test worker] stopping nodes
[22:12:28,466][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[22:12:28,467][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test] closing ... (reason [shutdown])
[22:12:28,467][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/GcBWwH6aRd-tTfW8CQOKcQ] closing index service (reason [shutdown])
[22:12:28,467][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[22:12:28,467][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:12:28,467][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:12:28,473][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:12:28,473][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:12:28,473][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:12:28,475][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:12:28,475][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:12:28,475][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[22:12:28,475][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[22:12:28,475][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:12:28,475][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:12:28,479][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:12:28,479][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:12:28,480][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:12:28,480][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:12:28,480][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:12:28,480][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[22:12:28,480][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[22:12:28,480][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:12:28,480][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:12:28,485][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:12:28,485][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:12:28,485][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:12:28,486][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:12:28,486][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:12:28,486][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[22:12:28,486][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[22:12:28,486][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:12:28,486][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:12:28,486][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:12:28,486][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:12:28,487][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:12:28,487][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:12:28,487][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:12:28,487][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[22:12:28,487][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[22:12:28,487][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:12:28,487][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:12:28,491][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:12:28,491][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:12:28,491][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:12:28,491][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:12:28,491][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:12:28,491][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[22:12:28,491][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:12:28,491][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[22:12:28,491][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:12:28,492][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/GcBWwH6aRd-tTfW8CQOKcQ] closed... (reason [shutdown])
[22:12:28,492][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[22:12:28,492][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[22:12:28,493][INFO ][org.elasticsearch.node.Node][Test worker] closed
[22:12:28,497][INFO ][test                     ][Test worker] data files wiped
[22:12:30,499][INFO ][test                     ][Test worker] settings cluster name
[22:12:30,500][INFO ][test                     ][Test worker] starting nodes
[22:12:30,500][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[22:12:30,501][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[22:12:30,511][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[22:12:30,512][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [138.5gb], usable_space [138.3gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[22:12:30,512][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[22:12:30,512][INFO ][org.elasticsearch.node.Node][Test worker] node name [XpnAVjp] derived from node ID [XpnAVjpKTbeo3oBFIRiiKQ]; set [node.name] to override
[22:12:30,512][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[5199], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[22:12:30,513][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[22:12:30,513][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[22:12:30,513][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[22:12:30,513][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[22:12:30,513][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[22:12:30,513][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[22:12:30,514][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[22:12:30,514][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[22:12:30,514][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[22:12:30,514][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[22:12:30,514][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[22:12:30,514][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[22:12:30,514][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[22:12:30,514][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[22:12:30,514][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[22:12:30,515][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[22:12:30,515][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[22:12:30,515][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[22:12:30,515][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[22:12:30,517][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:45e7:7aff:1af8:a30a prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[22:12:30,518][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[22:12:30,518][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[22:12:30,518][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[22:12:30,518][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[22:12:30,518][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[22:12:30,518][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[22:12:30,518][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[22:12:30,519][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[22:12:30,520][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[22:12:30,520][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[22:12:30,520][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[22:12:30,520][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[22:12:30,521][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[22:12:30,521][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[22:12:30,521][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[22:12:30,521][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:12:30,521][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:12:30,548][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[22:12:30,553][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[22:12:30,731][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[22:12:30,735][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[22:12:30,736][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[22:12:30,737][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[22:12:30,737][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[8]}, bound_addresses {local[8]}
[22:12:30,739][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[22:12:30,740][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [initial_join]: execute
[22:12:30,740][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[22:12:33,743][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[XpnAVjp][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]}], id[56], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[22:12:33,743][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[XpnAVjp][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[22:12:33,743][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[22:12:33,744][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[22:12:33,744][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] new_master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[22:12:33,744][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] publishing cluster state version [1]
[22:12:33,744][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] set local cluster state to version 1
[22:12:33,746][INFO ][org.elasticsearch.node.Node][Test worker] started
[22:12:33,746][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [2ms] done applying updated cluster_state (version: 1, uuid: 3Ku4LJznTnuuZI2tgA0sAw)
[22:12:33,748][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[22:12:33,749][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[22:12:33,749][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] publishing cluster state version [2]
[22:12:33,749][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] set local cluster state to version 2
[22:12:33,751][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[22:12:33,751][INFO ][test                     ][Test worker] nodes are started
[22:12:33,751][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [2ms] done applying updated cluster_state (version: 2, uuid: dNpS97aSSWmheisU4EXxIg)
[22:12:33,751][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [create-index [test], cause [api]]: execute
[22:12:33,752][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] creating Index [[test/iF1gWk_PRhqGGTlPJIsxDQ]], shards [5]/[1] - reason [create index]
[22:12:33,752][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:12:34,222][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] using dynamic[true]
[22:12:34,224][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test] creating index, cause [api], templates [], shards [5]/[1], mappings [type1]
[22:12:34,225][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[22:12:34,225][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test/iF1gWk_PRhqGGTlPJIsxDQ] closing index service (reason [cleaning up after validating index on master])
[22:12:34,225][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:12:34,225][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] full cache clear, reason [close]
[22:12:34,225][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:12:34,225][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test/iF1gWk_PRhqGGTlPJIsxDQ] closed... (reason [cleaning up after validating index on master])
[22:12:34,226][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [api]]
[22:12:34,226][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] publishing cluster state version [3]
[22:12:34,226][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] set local cluster state to version 3
[22:12:34,226][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [[test/iF1gWk_PRhqGGTlPJIsxDQ]] creating index
[22:12:34,226][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] creating Index [[test/iF1gWk_PRhqGGTlPJIsxDQ]], shards [5]/[1] - reason [create index]
[22:12:34,226][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:12:34,689][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] using dynamic[true]
[22:12:34,689][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [[test/iF1gWk_PRhqGGTlPJIsxDQ]] adding mapping [type1], source [{&quot;type1&quot;:{&quot;properties&quot;:{&quot;points&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;store&quot;:true,&quot;fields&quot;:{&quot;sort&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;:&quot;naturalsort&quot;,&quot;fielddata&quot;:true}}}}}}]
[22:12:34,691][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][1] creating shard
[22:12:34,691][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iF1gWk_PRhqGGTlPJIsxDQ/1, shard=[test][1]}]
[22:12:34,691][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] creating shard_id [test][1]
[22:12:34,692][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:12:34,692][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [CREATED]
[22:12:34,693][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:12:34,693][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][3] creating shard
[22:12:34,693][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#3]] starting recovery from store ...
[22:12:34,693][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iF1gWk_PRhqGGTlPJIsxDQ/3, shard=[test][3]}]
[22:12:34,693][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] creating shard_id [test][3]
[22:12:34,694][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:12:34,694][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [CREATED]
[22:12:34,695][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:12:34,696][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][2] creating shard
[22:12:34,696][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XpnAVjp][generic][T#3]] wipe translog location - creating new translog
[22:12:34,696][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#1]] starting recovery from store ...
[22:12:34,696][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iF1gWk_PRhqGGTlPJIsxDQ/2, shard=[test][2]}]
[22:12:34,696][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] creating shard_id [test][2]
[22:12:34,697][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XpnAVjp][generic][T#3]] no translog ID present in the current generation - creating one
[22:12:34,697][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:12:34,697][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [CREATED]
[22:12:34,697][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XpnAVjp][generic][T#1]] wipe translog location - creating new translog
[22:12:34,698][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:12:34,698][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][0] creating shard
[22:12:34,698][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XpnAVjp][generic][T#1]] no translog ID present in the current generation - creating one
[22:12:34,698][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#2]] starting recovery from store ...
[22:12:34,698][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iF1gWk_PRhqGGTlPJIsxDQ/0, shard=[test][0]}]
[22:12:34,698][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] creating shard_id [test][0]
[22:12:34,699][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:12:34,699][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [CREATED]
[22:12:34,699][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XpnAVjp][generic][T#2]] wipe translog location - creating new translog
[22:12:34,700][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:12:34,700][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:12:34,700][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#4]] starting recovery from store ...
[22:12:34,700][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#3]] recovery completed from [shard_store], took [9ms]
[22:12:34,700][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [XpnAVjpKTbeo3oBFIRiiKQ] for shard entry [shard id [[test][1]], allocation id [mmLF0zPSRZu7lowwxktA8w], primary term [0], message [after new shard recovery]]
[22:12:34,701][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [mmLF0zPSRZu7lowwxktA8w], primary term [0], message [after new shard recovery]]
[22:12:34,701][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XpnAVjp][generic][T#2]] no translog ID present in the current generation - creating one
[22:12:34,701][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:12:34,701][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#1]] recovery completed from [shard_store], took [7ms]
[22:12:34,701][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][generic][T#1]] [test][3] sending [internal:cluster/shard/started] to [XpnAVjpKTbeo3oBFIRiiKQ] for shard entry [shard id [[test][3]], allocation id [qINP0ZwjSOeX19NeqAjQXQ], primary term [0], message [after new shard recovery]]
[22:12:34,701][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][generic][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [qINP0ZwjSOeX19NeqAjQXQ], primary term [0], message [after new shard recovery]]
[22:12:34,702][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XpnAVjp][generic][T#4]] wipe translog location - creating new translog
[22:12:34,702][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [create-index [test], cause [api]]: took [950ms] done applying updated cluster_state (version: 3, uuid: LPjrHdYDQqyil8KkNeu8dg)
[22:12:34,702][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [mmLF0zPSRZu7lowwxktA8w], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [qINP0ZwjSOeX19NeqAjQXQ], primary term [0], message [after new shard recovery]]]: execute
[22:12:34,702][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[XpnAVjpKTbeo3oBFIRiiKQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=mmLF0zPSRZu7lowwxktA8w], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:12:34.224Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [mmLF0zPSRZu7lowwxktA8w], primary term [0], message [after new shard recovery]])
[22:12:34,702][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[XpnAVjpKTbeo3oBFIRiiKQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=qINP0ZwjSOeX19NeqAjQXQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:12:34.224Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [qINP0ZwjSOeX19NeqAjQXQ], primary term [0], message [after new shard recovery]])
[22:12:34,703][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XpnAVjp][generic][T#4]] no translog ID present in the current generation - creating one
[22:12:34,704][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:12:34,704][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#2]] recovery completed from [shard_store], took [8ms]
[22:12:34,704][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][generic][T#2]] [test][2] sending [internal:cluster/shard/started] to [XpnAVjpKTbeo3oBFIRiiKQ] for shard entry [shard id [[test][2]], allocation id [nNioBUZyQxShcABJ3h2Eyg], primary term [0], message [after new shard recovery]]
[22:12:34,704][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [mmLF0zPSRZu7lowwxktA8w], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [qINP0ZwjSOeX19NeqAjQXQ], primary term [0], message [after new shard recovery]]]
[22:12:34,704][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] publishing cluster state version [4]
[22:12:34,704][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][generic][T#2]] [test][2] received shard started for [shard id [[test][2]], allocation id [nNioBUZyQxShcABJ3h2Eyg], primary term [0], message [after new shard recovery]]
[22:12:34,704][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] set local cluster state to version 4
[22:12:34,706][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:12:34,706][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:12:34,706][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#4]] recovery completed from [shard_store], took [7ms]
[22:12:34,706][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [XpnAVjpKTbeo3oBFIRiiKQ] for shard entry [shard id [[test][0]], allocation id [3I_7rXvvT0K3nXDkawiYbw], primary term [0], message [after new shard recovery]]
[22:12:34,707][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:12:34,707][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [3I_7rXvvT0K3nXDkawiYbw], primary term [0], message [after new shard recovery]]
[22:12:34,707][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][2] sending [internal:cluster/shard/started] to [XpnAVjpKTbeo3oBFIRiiKQ] for shard entry [shard id [[test][2]], allocation id [nNioBUZyQxShcABJ3h2Eyg], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:12:34,707][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [nNioBUZyQxShcABJ3h2Eyg], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:12:34,707][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][4] creating shard
[22:12:34,707][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iF1gWk_PRhqGGTlPJIsxDQ/4, shard=[test][4]}]
[22:12:34,707][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] creating shard_id [test][4]
[22:12:34,708][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:12:34,708][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [CREATED]
[22:12:34,709][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:12:34,709][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#3]] starting recovery from store ...
[22:12:34,709][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [XpnAVjpKTbeo3oBFIRiiKQ] for shard entry [shard id [[test][0]], allocation id [3I_7rXvvT0K3nXDkawiYbw], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:12:34,709][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [3I_7rXvvT0K3nXDkawiYbw], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:12:34,710][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [mmLF0zPSRZu7lowwxktA8w], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [qINP0ZwjSOeX19NeqAjQXQ], primary term [0], message [after new shard recovery]]]: took [8ms] done applying updated cluster_state (version: 4, uuid: 1hwczgzsQK6rLdpTnr6LEA)
[22:12:34,710][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[XpnAVjp][generic][T#3]] wipe translog location - creating new translog
[22:12:34,711][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [nNioBUZyQxShcABJ3h2Eyg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [3I_7rXvvT0K3nXDkawiYbw], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [nNioBUZyQxShcABJ3h2Eyg], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [3I_7rXvvT0K3nXDkawiYbw], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:12:34,711][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[XpnAVjpKTbeo3oBFIRiiKQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=nNioBUZyQxShcABJ3h2Eyg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:12:34.224Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [nNioBUZyQxShcABJ3h2Eyg], primary term [0], message [after new shard recovery]])
[22:12:34,712][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[XpnAVjpKTbeo3oBFIRiiKQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=3I_7rXvvT0K3nXDkawiYbw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:12:34.224Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [3I_7rXvvT0K3nXDkawiYbw], primary term [0], message [after new shard recovery]])
[22:12:34,712][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[XpnAVjp][generic][T#3]] no translog ID present in the current generation - creating one
[22:12:34,713][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][2]], allocation id [nNioBUZyQxShcABJ3h2Eyg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [3I_7rXvvT0K3nXDkawiYbw], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [nNioBUZyQxShcABJ3h2Eyg], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [3I_7rXvvT0K3nXDkawiYbw], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:12:34,713][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] publishing cluster state version [5]
[22:12:34,713][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] set local cluster state to version 5
[22:12:34,714][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:12:34,714][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:12:34,714][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][generic][T#3]] recovery completed from [shard_store], took [7ms]
[22:12:34,714][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [XpnAVjpKTbeo3oBFIRiiKQ] for shard entry [shard id [[test][4]], allocation id [CCYmSXM9SVuU52aUBSejMQ], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:12:34,714][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [XpnAVjpKTbeo3oBFIRiiKQ] for shard entry [shard id [[test][4]], allocation id [CCYmSXM9SVuU52aUBSejMQ], primary term [0], message [after new shard recovery]]
[22:12:34,715][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [CCYmSXM9SVuU52aUBSejMQ], primary term [0], message [after new shard recovery]]
[22:12:34,715][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [CCYmSXM9SVuU52aUBSejMQ], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:12:34,715][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:12:34,716][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [nNioBUZyQxShcABJ3h2Eyg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [3I_7rXvvT0K3nXDkawiYbw], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [nNioBUZyQxShcABJ3h2Eyg], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [3I_7rXvvT0K3nXDkawiYbw], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [5ms] done applying updated cluster_state (version: 5, uuid: 4MAMQEFdRnWSzKN-kJiK8Q)
[22:12:34,716][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [CCYmSXM9SVuU52aUBSejMQ], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [CCYmSXM9SVuU52aUBSejMQ], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:12:34,716][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[XpnAVjpKTbeo3oBFIRiiKQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=CCYmSXM9SVuU52aUBSejMQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:12:34.224Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [CCYmSXM9SVuU52aUBSejMQ], primary term [0], message [after new shard recovery]])
[22:12:34,717][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [CCYmSXM9SVuU52aUBSejMQ], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [CCYmSXM9SVuU52aUBSejMQ], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:12:34,717][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] publishing cluster state version [6]
[22:12:34,717][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] set local cluster state to version 6
[22:12:34,717][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:12:34,718][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[XpnAVjp][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [CCYmSXM9SVuU52aUBSejMQ], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [CCYmSXM9SVuU52aUBSejMQ], primary term [0], message [master {XpnAVjp}{XpnAVjpKTbeo3oBFIRiiKQ}{3FZOZ8LTR9-NA0DNtN15nQ}{local}{local[8]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 6, uuid: HynY6ljtTICh_amLGoFUhw)
[22:13:03,749][INFO ][org.elasticsearch.cluster.routing.allocation.DiskThresholdMonitor][elasticsearch[XpnAVjp][management][T#2]] low disk watermark [85%] exceeded on [XpnAVjpKTbeo3oBFIRiiKQ][XpnAVjp][/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0] free: 138.3gb[14.8%], replicas will not be assigned to this node
[22:13:04,756][INFO ][test                     ][Test worker] stopping nodes
[22:13:04,756][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[22:13:04,757][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test] closing ... (reason [shutdown])
[22:13:04,757][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/iF1gWk_PRhqGGTlPJIsxDQ] closing index service (reason [shutdown])
[22:13:04,757][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[22:13:04,757][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:04,757][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:04,757][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:04,757][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:04,757][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:04,758][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:04,758][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:04,758][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[22:13:04,758][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[22:13:04,758][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:04,758][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:04,758][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:04,758][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:04,758][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:04,759][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:04,759][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:04,759][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[22:13:04,759][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[22:13:04,759][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:04,759][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:04,763][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:04,764][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:04,764][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:04,765][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:04,765][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:04,765][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[22:13:04,765][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[22:13:04,765][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:04,765][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:04,765][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:04,765][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:04,765][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:04,766][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:04,766][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:04,766][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[22:13:04,766][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[22:13:04,766][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:04,766][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:04,771][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:04,771][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:04,771][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:04,772][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:04,772][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:04,772][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[22:13:04,772][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:13:04,772][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[22:13:04,772][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:13:04,772][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/iF1gWk_PRhqGGTlPJIsxDQ] closed... (reason [shutdown])
[22:13:04,772][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[22:13:04,772][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[22:13:04,773][INFO ][org.elasticsearch.node.Node][Test worker] closed
[22:13:04,778][INFO ][test                     ][Test worker] data files wiped
[22:13:06,779][INFO ][test                     ][Test worker] settings cluster name
[22:13:06,779][INFO ][test                     ][Test worker] starting nodes
[22:13:06,779][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[22:13:06,780][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[22:13:06,786][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[22:13:06,786][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [138.5gb], usable_space [138.3gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[22:13:06,786][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[22:13:06,787][INFO ][org.elasticsearch.node.Node][Test worker] node name [VgamD9r] derived from node ID [VgamD9rmRZmm3TEXlMYU8A]; set [node.name] to override
[22:13:06,787][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[5199], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[22:13:06,787][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[22:13:06,787][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[22:13:06,787][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[22:13:06,787][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[22:13:06,788][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[22:13:06,788][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[22:13:06,788][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[22:13:06,788][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[22:13:06,788][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[22:13:06,788][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[22:13:06,788][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[22:13:06,789][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[22:13:06,789][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[22:13:06,789][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[22:13:06,789][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[22:13:06,789][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[22:13:06,789][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[22:13:06,789][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[22:13:06,790][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[22:13:06,791][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:45e7:7aff:1af8:a30a prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[22:13:06,791][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[22:13:06,792][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[22:13:06,792][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[22:13:06,792][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[22:13:06,792][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[22:13:06,792][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[22:13:06,792][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[22:13:06,792][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[22:13:06,793][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[22:13:06,793][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[22:13:06,794][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[22:13:06,794][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[22:13:06,794][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[22:13:06,794][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[22:13:06,794][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[22:13:06,794][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:13:06,795][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[22:13:06,820][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[22:13:06,825][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[22:13:07,004][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[22:13:07,008][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[22:13:07,009][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[22:13:07,009][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[22:13:07,009][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[9]}, bound_addresses {local[9]}
[22:13:07,010][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[22:13:07,010][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [initial_join]: execute
[22:13:07,010][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[22:13:10,014][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[VgamD9r][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]}], id[63], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[22:13:10,015][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[VgamD9r][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[22:13:10,015][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[22:13:10,015][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[22:13:10,015][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] new_master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[22:13:10,015][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] publishing cluster state version [1]
[22:13:10,016][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] set local cluster state to version 1
[22:13:10,016][INFO ][org.elasticsearch.node.Node][Test worker] started
[22:13:10,017][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [1ms] done applying updated cluster_state (version: 1, uuid: totyrQfuRR6okgFA_AOvXQ)
[22:13:10,019][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[22:13:10,020][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[22:13:10,020][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] publishing cluster state version [2]
[22:13:10,020][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] set local cluster state to version 2
[22:13:10,023][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[22:13:10,023][INFO ][test                     ][Test worker] nodes are started
[22:13:10,023][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [3ms] done applying updated cluster_state (version: 2, uuid: Mb6Nm4dJSMuxSt46jrMlHg)
[22:13:10,023][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [create-index [test], cause [api]]: execute
[22:13:10,024][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] creating Index [[test/JoonlFXVSKCDZakMWT3Z6Q]], shards [5]/[1] - reason [create index]
[22:13:10,024][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:13:10,504][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] using dynamic[true]
[22:13:10,507][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test] creating index, cause [api], templates [], shards [5]/[1], mappings [type1]
[22:13:10,509][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[22:13:10,510][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test/JoonlFXVSKCDZakMWT3Z6Q] closing index service (reason [cleaning up after validating index on master])
[22:13:10,510][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:13:10,510][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] full cache clear, reason [close]
[22:13:10,510][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[22:13:10,510][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test/JoonlFXVSKCDZakMWT3Z6Q] closed... (reason [cleaning up after validating index on master])
[22:13:10,511][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [api]]
[22:13:10,511][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] publishing cluster state version [3]
[22:13:10,511][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] set local cluster state to version 3
[22:13:10,511][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [[test/JoonlFXVSKCDZakMWT3Z6Q]] creating index
[22:13:10,511][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] creating Index [[test/JoonlFXVSKCDZakMWT3Z6Q]], shards [5]/[1] - reason [create index]
[22:13:10,512][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[22:13:10,945][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] using dynamic[true]
[22:13:10,945][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [[test/JoonlFXVSKCDZakMWT3Z6Q]] adding mapping [type1], source [{&quot;type1&quot;:{&quot;properties&quot;:{&quot;points&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;store&quot;:true,&quot;fields&quot;:{&quot;sort&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;:&quot;naturalsort&quot;,&quot;fielddata&quot;:true}}}}}}]
[22:13:10,947][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][1] creating shard
[22:13:10,947][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/JoonlFXVSKCDZakMWT3Z6Q/1, shard=[test][1]}]
[22:13:10,947][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] creating shard_id [test][1]
[22:13:10,947][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:10,948][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:10,949][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:10,949][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][3] creating shard
[22:13:10,949][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#3]] starting recovery from store ...
[22:13:10,950][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/JoonlFXVSKCDZakMWT3Z6Q/3, shard=[test][3]}]
[22:13:10,950][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] creating shard_id [test][3]
[22:13:10,951][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:10,951][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:10,951][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[VgamD9r][generic][T#3]] wipe translog location - creating new translog
[22:13:10,952][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[VgamD9r][generic][T#3]] no translog ID present in the current generation - creating one
[22:13:10,953][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:10,953][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][2] creating shard
[22:13:10,953][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#1]] starting recovery from store ...
[22:13:10,953][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/JoonlFXVSKCDZakMWT3Z6Q/2, shard=[test][2]}]
[22:13:10,953][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] creating shard_id [test][2]
[22:13:10,954][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:10,954][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:10,955][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[VgamD9r][generic][T#1]] wipe translog location - creating new translog
[22:13:10,955][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:10,955][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][0] creating shard
[22:13:10,955][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#2]] starting recovery from store ...
[22:13:10,956][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[VgamD9r][generic][T#1]] no translog ID present in the current generation - creating one
[22:13:10,956][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/JoonlFXVSKCDZakMWT3Z6Q/0, shard=[test][0]}]
[22:13:10,956][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] creating shard_id [test][0]
[22:13:10,956][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:10,956][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#3]] recovery completed from [shard_store], took [9ms]
[22:13:10,957][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [VgamD9rmRZmm3TEXlMYU8A] for shard entry [shard id [[test][1]], allocation id [-fcNeG1ZT8qdVwzYq8XIVg], primary term [0], message [after new shard recovery]]
[22:13:10,957][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:10,957][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [-fcNeG1ZT8qdVwzYq8XIVg], primary term [0], message [after new shard recovery]]
[22:13:10,957][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:10,957][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[VgamD9r][generic][T#2]] wipe translog location - creating new translog
[22:13:10,958][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:10,958][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[VgamD9r][generic][T#2]] no translog ID present in the current generation - creating one
[22:13:10,958][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#4]] starting recovery from store ...
[22:13:10,959][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:10,959][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#1]] recovery completed from [shard_store], took [9ms]
[22:13:10,959][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][generic][T#1]] [test][3] sending [internal:cluster/shard/started] to [VgamD9rmRZmm3TEXlMYU8A] for shard entry [shard id [[test][3]], allocation id [tzvcf6NJT2yob4Cu3I6SeQ], primary term [0], message [after new shard recovery]]
[22:13:10,959][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][generic][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [tzvcf6NJT2yob4Cu3I6SeQ], primary term [0], message [after new shard recovery]]
[22:13:10,959][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [create-index [test], cause [api]]: took [935ms] done applying updated cluster_state (version: 3, uuid: h_Za39v3TFuBbgV7PmUjQw)
[22:13:10,959][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [-fcNeG1ZT8qdVwzYq8XIVg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [tzvcf6NJT2yob4Cu3I6SeQ], primary term [0], message [after new shard recovery]]]: execute
[22:13:10,959][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[VgamD9rmRZmm3TEXlMYU8A], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=-fcNeG1ZT8qdVwzYq8XIVg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:10.508Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [-fcNeG1ZT8qdVwzYq8XIVg], primary term [0], message [after new shard recovery]])
[22:13:10,959][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[VgamD9r][generic][T#4]] wipe translog location - creating new translog
[22:13:10,959][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[VgamD9rmRZmm3TEXlMYU8A], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=tzvcf6NJT2yob4Cu3I6SeQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:10.508Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [tzvcf6NJT2yob4Cu3I6SeQ], primary term [0], message [after new shard recovery]])
[22:13:10,960][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[VgamD9r][generic][T#4]] no translog ID present in the current generation - creating one
[22:13:10,960][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:10,960][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#2]] recovery completed from [shard_store], took [7ms]
[22:13:10,960][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][generic][T#2]] [test][2] sending [internal:cluster/shard/started] to [VgamD9rmRZmm3TEXlMYU8A] for shard entry [shard id [[test][2]], allocation id [X3sLaUUCTWecX8gD3Fx8eA], primary term [0], message [after new shard recovery]]
[22:13:10,961][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][generic][T#2]] [test][2] received shard started for [shard id [[test][2]], allocation id [X3sLaUUCTWecX8gD3Fx8eA], primary term [0], message [after new shard recovery]]
[22:13:10,961][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [-fcNeG1ZT8qdVwzYq8XIVg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [tzvcf6NJT2yob4Cu3I6SeQ], primary term [0], message [after new shard recovery]]]
[22:13:10,961][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] publishing cluster state version [4]
[22:13:10,961][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] set local cluster state to version 4
[22:13:10,962][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:10,963][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:10,963][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#4]] recovery completed from [shard_store], took [6ms]
[22:13:10,963][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:10,963][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [VgamD9rmRZmm3TEXlMYU8A] for shard entry [shard id [[test][0]], allocation id [pyT0J7I8S3S4ol4EMAuVCw], primary term [0], message [after new shard recovery]]
[22:13:10,963][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][2] sending [internal:cluster/shard/started] to [VgamD9rmRZmm3TEXlMYU8A] for shard entry [shard id [[test][2]], allocation id [X3sLaUUCTWecX8gD3Fx8eA], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:10,963][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [pyT0J7I8S3S4ol4EMAuVCw], primary term [0], message [after new shard recovery]]
[22:13:10,963][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [X3sLaUUCTWecX8gD3Fx8eA], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:10,963][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][4] creating shard
[22:13:10,964][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/JoonlFXVSKCDZakMWT3Z6Q/4, shard=[test][4]}]
[22:13:10,964][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] creating shard_id [test][4]
[22:13:10,964][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[22:13:10,964][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [CREATED]
[22:13:10,965][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[22:13:10,965][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#3]] starting recovery from store ...
[22:13:10,965][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [VgamD9rmRZmm3TEXlMYU8A] for shard entry [shard id [[test][0]], allocation id [pyT0J7I8S3S4ol4EMAuVCw], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:10,966][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [pyT0J7I8S3S4ol4EMAuVCw], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:10,967][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[VgamD9r][generic][T#3]] wipe translog location - creating new translog
[22:13:10,968][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [-fcNeG1ZT8qdVwzYq8XIVg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [tzvcf6NJT2yob4Cu3I6SeQ], primary term [0], message [after new shard recovery]]]: took [8ms] done applying updated cluster_state (version: 4, uuid: 7MaP7nq9SiCcZQjRhWBnUw)
[22:13:10,968][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [X3sLaUUCTWecX8gD3Fx8eA], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [pyT0J7I8S3S4ol4EMAuVCw], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [X3sLaUUCTWecX8gD3Fx8eA], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [pyT0J7I8S3S4ol4EMAuVCw], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[22:13:10,968][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[VgamD9r][generic][T#3]] no translog ID present in the current generation - creating one
[22:13:10,968][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[VgamD9rmRZmm3TEXlMYU8A], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=X3sLaUUCTWecX8gD3Fx8eA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:10.508Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [X3sLaUUCTWecX8gD3Fx8eA], primary term [0], message [after new shard recovery]])
[22:13:10,968][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[VgamD9rmRZmm3TEXlMYU8A], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=pyT0J7I8S3S4ol4EMAuVCw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:10.508Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [pyT0J7I8S3S4ol4EMAuVCw], primary term [0], message [after new shard recovery]])
[22:13:10,969][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][2]], allocation id [X3sLaUUCTWecX8gD3Fx8eA], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [pyT0J7I8S3S4ol4EMAuVCw], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [X3sLaUUCTWecX8gD3Fx8eA], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [pyT0J7I8S3S4ol4EMAuVCw], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[22:13:10,969][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] publishing cluster state version [5]
[22:13:10,970][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] set local cluster state to version 5
[22:13:10,970][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[22:13:10,971][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:10,971][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][generic][T#3]] recovery completed from [shard_store], took [7ms]
[22:13:10,971][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [VgamD9rmRZmm3TEXlMYU8A] for shard entry [shard id [[test][4]], allocation id [aAuap-TRQpKZLsq6M8Xbow], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:10,971][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [VgamD9rmRZmm3TEXlMYU8A] for shard entry [shard id [[test][4]], allocation id [aAuap-TRQpKZLsq6M8Xbow], primary term [0], message [after new shard recovery]]
[22:13:10,971][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [aAuap-TRQpKZLsq6M8Xbow], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[22:13:10,971][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [aAuap-TRQpKZLsq6M8Xbow], primary term [0], message [after new shard recovery]]
[22:13:10,971][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:10,972][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [X3sLaUUCTWecX8gD3Fx8eA], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [pyT0J7I8S3S4ol4EMAuVCw], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [X3sLaUUCTWecX8gD3Fx8eA], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [pyT0J7I8S3S4ol4EMAuVCw], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 5, uuid: O32oGGA4RrSxrytWpkGnKg)
[22:13:10,972][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [aAuap-TRQpKZLsq6M8Xbow], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][4]], allocation id [aAuap-TRQpKZLsq6M8Xbow], primary term [0], message [after new shard recovery]]]: execute
[22:13:10,973][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[VgamD9rmRZmm3TEXlMYU8A], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=aAuap-TRQpKZLsq6M8Xbow], unassigned_info[[reason=INDEX_CREATED], at[2017-01-25T21:13:10.508Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [aAuap-TRQpKZLsq6M8Xbow], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]])
[22:13:10,974][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [aAuap-TRQpKZLsq6M8Xbow], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][4]], allocation id [aAuap-TRQpKZLsq6M8Xbow], primary term [0], message [after new shard recovery]]]
[22:13:10,974][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] publishing cluster state version [6]
[22:13:10,975][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] set local cluster state to version 6
[22:13:10,976][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[22:13:10,977][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[VgamD9r][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [aAuap-TRQpKZLsq6M8Xbow], primary term [0], message [master {VgamD9r}{VgamD9rmRZmm3TEXlMYU8A}{rmjpEjG2TaisNN1eCbAYZQ}{local}{local[9]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][4]], allocation id [aAuap-TRQpKZLsq6M8Xbow], primary term [0], message [after new shard recovery]]]: took [4ms] done applying updated cluster_state (version: 6, uuid: OW-krrnjRFClGCUxZyw1Jg)
[22:13:40,022][INFO ][org.elasticsearch.cluster.routing.allocation.DiskThresholdMonitor][elasticsearch[VgamD9r][management][T#2]] low disk watermark [85%] exceeded on [VgamD9rmRZmm3TEXlMYU8A][VgamD9r][/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0] free: 138.3gb[14.8%], replicas will not be assigned to this node
[22:13:41,031][INFO ][test                     ][Test worker] stopping nodes
[22:13:41,031][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[22:13:41,032][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test] closing ... (reason [shutdown])
[22:13:41,032][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/JoonlFXVSKCDZakMWT3Z6Q] closing index service (reason [shutdown])
[22:13:41,032][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[22:13:41,032][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:41,032][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:41,038][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:41,038][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:41,038][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:41,039][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:41,040][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:41,040][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[22:13:41,040][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[22:13:41,040][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:41,040][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:41,044][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:41,044][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:41,044][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:41,045][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:41,045][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:41,045][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[22:13:41,045][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[22:13:41,045][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:41,045][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:41,045][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:41,045][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:41,046][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:41,046][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:41,046][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:41,046][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[22:13:41,046][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[22:13:41,046][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:41,046][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:41,046][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:41,046][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:41,047][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:41,047][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:41,047][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:41,047][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[22:13:41,047][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[22:13:41,047][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[22:13:41,047][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[22:13:41,051][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[22:13:41,051][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[22:13:41,051][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[22:13:41,052][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[22:13:41,052][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[22:13:41,052][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[22:13:41,052][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:13:41,052][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[22:13:41,052][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[22:13:41,052][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/JoonlFXVSKCDZakMWT3Z6Q] closed... (reason [shutdown])
[22:13:41,052][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[22:13:41,052][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[22:13:41,053][INFO ][org.elasticsearch.node.Node][Test worker] closed
[22:13:41,058][INFO ][test                     ][Test worker] data files wiped
</pre>
</span>
</div>
</div>
<div id="footer">
<p>
<div>
<label class="hidden" id="label-for-line-wrapping-toggle" for="line-wrapping-toggle">Wrap lines
<input id="line-wrapping-toggle" type="checkbox" autocomplete="off"/>
</label>
</div>Generated by 
<a href="http://www.gradle.org">Gradle 3.2.1</a> at 25.01.2017 22:14:33</p>
</div>
</div>
</body>
</html>
